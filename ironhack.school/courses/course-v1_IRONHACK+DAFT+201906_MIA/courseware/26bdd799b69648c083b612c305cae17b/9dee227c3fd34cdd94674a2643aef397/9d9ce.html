







<!DOCTYPE html>
<!--[if lte IE 9]><html class="ie ie9 lte9" lang="en"><![endif]-->
<!--[if !IE]><!--><html lang="en"><!--<![endif]-->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head dir="ltr">
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">



  
<title data-base-title="
 Inferential Statistics in Python | DAFT Courseware | Ironhack Student Platform">
  
 Introduction to Bayesian Statistics | Inferential Statistics in Python | DAFT Courseware | Ironhack Student Platform
</title>


      <script type="text/javascript">
        /* immediately break out of an iframe if coming from the marketing website */
        (function(window) {
          if (window.location !== window.top.location) {
            window.top.location = window.location;
          }
        })(this);
      </script>

  

  <script type="text/javascript" src="../../../../../static/js/i18n/en/djangojs.e37eef1ffc63.js"></script>
  <script type="text/javascript" src="../../../../../static/js/ie11_find_array.bd1c6dc7a133.js"></script>

  <link rel="icon" type="image/x-icon" href="../../../../../static/ih-lms-theme/images/favicon.0de58aa5bfca.ico" />

  
  

    <link href="../../../../../static/ih-lms-theme/css/lms-style-vendor.68e48093f5dd.css" rel="stylesheet" type="text/css" />



      <link rel="stylesheet" href="../../../../../static/ih-lms-theme/css/bootstrap/lms-main.c54b2923d249.css" type="text/css" media="all" />

    
    <script type="text/javascript" src="../../../../../static/js/lms-main_vendor.a04b73033169.js" charset="utf-8"></script>


    
    <script type="text/javascript" src="../../../../../static/js/lms-application.e7bd4b65d083.js" charset="utf-8"></script>



  
    
    
    <script type="text/javascript" src="../../../../../static/bundles/commons.d60dcd98c024881d011e.c835e91d09f6.js" ></script>



  <script>
    window.baseUrl = "../../../../../static/index.html";
    (function (require) {
      require.config({
          baseUrl: window.baseUrl
      });
    }).call(this, require || RequireJS.require);
  </script>
  <script type="text/javascript" src="../../../../../static/lms/js/require-config.8e00198660b0.js"></script>
  
    <script type="text/javascript">
        (function (require) {
          require.config({
              paths: {
                'js/courseware/courseware_factory': 'js/courseware/courseware_factory.1504fc10caef',
'draggabilly': 'js/vendor/draggabilly.26caba6f7187',
'js/courseware/toggle_element_visibility': 'js/courseware/toggle_element_visibility.d5f10bc65ed0',
'hls': 'common/js/vendor/hls.b10b9ef4015b',
'js/courseware/link_clicked_events': 'js/courseware/link_clicked_events',
'moment': 'common/js/vendor/moment-with-locales.084396f4103c',
'moment-timezone': 'common/js/vendor/moment-timezone-with-data.60142e6c4416',
'js/student_account/logistration_factory': 'js/student_account/logistration_factory.d7765e4e6715',
'js/groups/views/cohorts_dashboard_factory': 'js/groups/views/cohorts_dashboard_factory.7f46663f97b4',
'js/dateutil_factory': 'js/dateutil_factory.841c29e02056',
'course_bookmarks/js/views/bookmark_button': 'course_bookmarks/js/views/bookmark_button.d4cfaf3361fa',
'js/courseware/accordion_events': 'js/courseware/accordion_events.6064c7809de5',
'js/views/message_banner': 'js/views/message_banner.af013d0ecbd4',
'js/groups/discussions_management/discussions_dashboard_factory': 'js/discussions_management/views/discussions_dashboard_factory.2e10d9097343',
'js/courseware/course_info_events': 'js/courseware/course_info_events.2fc35b57627f'
            }
          });
        }).call(this, require || RequireJS.require);
    </script>
  

  

  

    <link href="../../../../../static/ih-lms-theme/css/lms-style-course-vendor.691e50c64ad7.css" rel="stylesheet" type="text/css" />




  

    <link href="../../../../../static/ih-lms-theme/css/lms-course.1d0edf3ec473.css" rel="stylesheet" type="text/css" />




<script type="text/javascript" src="../../../../../static/js/jquery.autocomplete.3bd10d7510d2.js"></script>
<script type="text/javascript" src="../../../../../static/js/src/tooltip_manager.da362490e199.js"></script>

<link href="../../../../../static/css/vendor/jquery.autocomplete.896181d3ec33.css" rel="stylesheet" type="text/css">
  <script type="text/javascript" src="../../../../../static/bundles/XModuleShim.68b7a679342e3bdb698f.c506d949a49b.js" ></script>
<script type="text/javascript" src="../../../../../static/bundles/HtmlModule.174055b73c05dccfbd8b.823874f69fef.js" ></script>
<script type="text/javascript" src="../../../../../static/bundles/VerticalStudentView.6f1068648eb9e8d18da9.0a2f678d6564.js" ></script>
<script type="text/javascript" src="../../../../../static/bundles/SequenceModule.251564cf8963e6f00227.ef752443db97.js" ></script>

  

  


  


<script type="application/json" id="user-metadata">
    {"username": "NoahB", "enrollment_mode": "audit", "upgrade_link": null, "user_id": 489, "course_start": "2019-06-10T00:00:00+00:00", "upgrade_deadline": null, "course_key_fields": {"org": "IRONHACK", "run": "201906_MIA", "course": "DAFT"}, "course_end": "2019-08-19T00:00:00+00:00", "pacing_type": "instructor_paced", "upgrade_price": "Free", "course_id": "course-v1:IRONHACK+DAFT+201906_MIA", "has_staff_access": false, "forum_roles": [["Student"]], "enrollment_time": "2019-06-11T20:48:51.323840+00:00", "schedule_start": null, "email": "noahbeckerman@gmail.com"}
</script>

  

  


  


<!-- dummy Segment -->
<script type="text/javascript">
  var analytics = {
    track: function() { return; },
    trackLink: function() { return; },
    pageview: function() { return; },
    page: function() { return; }
  };
</script>
<!-- end dummy Segment -->


  <meta name="path_prefix" content="">
  
  

  <meta name="openedx-release-line" content="hawthorn" />






  <!-- Hotjar Tracking Code for https://ironhack.school -->
  <script type="text/javascript">
      (function(h,o,t,j,a,r){
          h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
          h._hjSettings={hjid:1193918,hjsv:6};
          a=o.getElementsByTagName('head')[0];
          r=o.createElement('script');r.async=1;
          r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
          a.appendChild(r);
      })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
  </script>

  <script type="text/javascript">
    function setCookieHideMobileWarning(expiration_hours) {
      var date = new Date();
      date.setTime(date.getTime()+(expiration_hours*60*60*1000));
      var expires = "; expires=" + date.toGMTString();
      document.cookie = "ihHideMobileWarning=true" + expires;
    }
    
    function isHideMobileWarningCookieSet() {
      return document.cookie.replace(/(?:(?:^|.*;\s*)ihHideMobileWarning\s*\=\s*([^;]*).*$)|^.*$/, "$1") !== "true"; 
    }

    function manageMobileWarning(mobile_warning_container) {
      if (isHideMobileWarningCookieSet()) {
        var mobile_warning_dismiss = document.getElementsByClassName('ih-js-hide-mobile-warning', mobile_warning_container)[0];
        mobile_warning_dismiss.addEventListener('click', function() {
          var expiration_hours = 48;
          setCookieHideMobileWarning(expiration_hours);
          mobile_warning_container.classList.add('hidden');
        });
      } else {
        mobile_warning_container.classList.add('hidden');
      }
    }

    var checkMobileWarningLayer = function () {
      var mobile_warning_container = document.getElementsByClassName('ih-mobile-warning-container')[0];
      (document.body && mobile_warning_container)? manageMobileWarning(mobile_warning_container) :  window.requestAnimationFrame(checkMobileWarningLayer);
    }

    window.requestAnimationFrame(checkMobileWarningLayer);
  </script>

</head>

<body class="ltr view-in-course view-courseware courseware  lang_en">

<div class="ih-mobile-warning-container">
  <img class="logo" src="../../../../../static/ih-lms-theme/images/logo.6230fc40336b.svg" />
  <div class="ih-mobile-warning-title">
    The Learning Platform is best experienced on Desktop devices
  </div>
  <div class="ih-mobile-warning-description">
    We recommend using a Desktop device to get the full experience of the Learning Platform.
  </div>
  <div class="ih-mobile-warning-dismiss ih-js-hide-mobile-warning">
   Continue anyway
  </div>
</div>


<div id="page-prompt"></div>
  <div class="window-wrap" dir="ltr">
    <a class="nav-skip sr-only sr-only-focusable" href="#main">Skip to main content</a>

    


            











<header class="global-header slim ih-global-header">
    <div class="ih-header-wrapper">
        <div class="main-header ih-main-header">
            





<h1 class="header-logo">
  <a href="../../../../../dashboard.html">
    
    <img  class="logo" src="../../../../../static/ih-lms-theme/images/logo.6230fc40336b.svg" alt="Ironhack Student Platform Home Page"/>
    
  </a>
    <div class="course-header">
          
      
      <span class="course-name">Data Analytics</span>
    </div>
</h1>

            <div class="hamburger-menu" role="button" aria-label=Options Menu aria-expanded="false" aria-controls="mobile-menu" tabindex="0">
                <span class="line"></span>
                <span class="line"></span>
                <span class="line"></span>
                <span class="line"></span>
            </div>
                










<div class="nav-links">
  <div class="main">
      
  </div>
  <div class="secondary ih-secondary">
    






  

<div class="nav-item hidden-mobile toggle-user-dropdown ih-username-container">
    <div class="menu-title">
        
        <span class="sr-only">Dashboard for:</span>
        <span class="username ih-username">Ironhack (NoahB)</span>
    </div>
</div>
<div class="nav-item hidden-mobile nav-item-dropdown ih-dropdown-wrapper" tabindex="-1">
    <div class="toggle-user-dropdown ih-toggle-user-dropdown" role="button" aria-label=Options Menu aria-expanded="false" tabindex="0" aria-controls="user-menu">
        <span class="fa fa-caret-down" aria-hidden="true"></span>
    </div>
    <div class="dropdown-user-menu hidden ih-dropdown-user-menu" aria-label=More Options role="menu" id="user-menu" tabindex="-1">
        <div class="mobile-nav-item dropdown-item dropdown-nav-item ih-dropdown-nav-item"><a href="../../../../../dashboard.html" role="menuitem">My courses</a></div>
            
        <div class="mobile-nav-item dropdown-item dropdown-nav-item ih-dropdown-nav-item"><a href="../../../../../account/settings.html" role="menuitem">Account</a></div>
        <div class="mobile-nav-item dropdown-item dropdown-nav-item ih-dropdown-nav-item"><a href="../../../../../index.html" role="menuitem">Sign Out</a></div>
    </div>
</div>
  </div>
</div>
        </div>
        <div class="mobile-menu hidden" aria-label=More Options role="menu" id="mobile-menu"></div>
    </div>    
</header>

<!--[if lte IE 9]>
<div class="ie-banner" aria-hidden="true"><strong>Warning:</strong> Your browser is not fully supported. We strongly recommend using <a href="https://www.google.com/chrome" target="_blank">Chrome</a> or <a href="http://www.mozilla.org/firefox" target="_blank">Firefox</a>.</div>
<![endif]-->




            








    










    
    

      <div 
          class="ih-content"
      >
        <div class="marketing-hero"></div>

        <div class="content-wrapper main-container ih-main-container" id="content">
          

















<script type="text/template" id="image-modal-tpl">
    <div class="wrapper-modal wrapper-modal-image">
  <section class="image-link">
    <%= smallHTML%>
    <a href="#" class="modal-ui-icon action-fullscreen" role="button">
      <span class="label">
        <span class="icon fa fa-arrows-alt fa-large" aria-hidden="true"></span> <%- gettext("Fullscreen") %>
      </span>
    </a>
  </section>

  <section class="image-modal">
    <section class="image-content">
      <div class="image-wrapper">
        <img alt="<%= largeALT %>, <%- gettext('Large') %>" src="<%= largeSRC %>" />
      </div>

      <a href="#" class="modal-ui-icon action-close" role="button">
        <span class="label">
          <span class="icon fa fa-remove fa-large" aria-hidden="true"></span> <%- gettext("Close") %>
        </span>
      </a>

      <ul class="image-controls">
        <li class="image-control">
          <a href="#" class="modal-ui-icon action-zoom-in" role="button">
            <span class="label">
              <span class="icon fa fa fa-search-plus fa-large" aria-hidden="true"></span> <%- gettext("Zoom In") %>
            </span>
          </a>
        </li>

        <li class="image-control">
          <a href="#" class="modal-ui-icon action-zoom-out is-disabled" aria-disabled="true" role="button">
            <span class="label">
              <span class="icon fa fa fa-search-minus fa-large" aria-hidden="true"></span> <%- gettext("Zoom Out") %>
            </span>
          </a>
        </li>
      </ul>
    </section>
  </section>
</div>

</script>








<div class="message-banner" aria-live="polite"></div>
  








    
        <nav class="navbar course-tabs pb-0 navbar-expand ih-navbar-container" aria-label="Course">
            <ul class="navbar-nav mr-auto ih-course-tabs">
                    
                    
                        <li class="nav-item ih-nav-item active">
                            <a href="../../../course/index.html" class="nav-link">
                                Course
                                    <span class="sr-only">, current location</span>
                            </a>
                        </li>
                    
                    
                        <li class="nav-item ih-nav-item ">
                            <a href="../../../discussion/forum/index.html" class="nav-link">
                                Discussion
                            </a>
                        </li>
                    
                    
            </ul>
        </nav>


<div class="container ih-course-container"
    lang="en"
  >
  <div class="course-wrapper ih-course-wrapper" role="presentation">

    <section class="course-content" id="course-content">
        <header class="page-header has-secondary">
            <div class="page-header-main">
                <nav aria-label="Course" class="sr-is-focusable" tabindex="-1">
                    <div class="has-breadcrumbs">
                        <div class="breadcrumbs">
                                <span class="nav-item nav-item-course">
                                    <a href="../../../course/index.html">Course</a>
                                </span>
                                <span class="icon fa fa-angle-right" aria-hidden="true"></span>
                                <span class="nav-item nav-item-chapter" data-course-position="2" data-chapter-position="3">
                                    <a href="../../../course/index.html#block-v1:IRONHACK+DAFT+201906_MIA+type@chapter+block@26bdd799b69648c083b612c305cae17b">Module 2</a>
                                </span>
                                <span class="icon fa fa-angle-right" aria-hidden="true"></span>
                                <span class="nav-item nav-item-section">
                                    <a href="../../../course/index.html#block-v1:IRONHACK+DAFT+201906_MIA+type@sequential+block@9dee227c3fd34cdd94674a2643aef397">Inferential Statistics in Python</a>
                                </span>
                                <span class="icon fa fa-angle-right" aria-hidden="true"></span>
                            <span class="nav-item nav-item-sequence">Introduction to Bayesian Statistics</span>
                        </div>
                    </div>
                </nav>
            </div>
        </header>

        <main id="main" tabindex="-1" aria-label="Content">

              <div class="xblock xblock-student_view xblock-student_view-sequential xmodule_display xmodule_SequenceModule" data-runtime-class="LmsRuntime" data-init="XBlockToXModuleShim" data-block-type="sequential" data-request-token="655e2bf8939911e99a866e487447f8c8" data-runtime-version="1" data-usage-id="block-v1:IRONHACK+DAFT+201906_MIA+type@sequential+block@9dee227c3fd34cdd94674a2643aef397" data-type="Sequence" data-course-id="course-v1:IRONHACK+DAFT+201906_MIA">
  <script type="json/xblock-args" class="xblock-json-init-args">
    {"xmodule-type": "Sequence"}
  </script>
  


<div id="sequence_9dee227c3fd34cdd94674a2643aef397" class="sequence" data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@sequential+block@9dee227c3fd34cdd94674a2643aef397" data-position="9" data-ajax-url="/courses/course-v1:IRONHACK+DAFT+201906_MIA/xblock/block-v1:IRONHACK+DAFT+201906_MIA+type@sequential+block@9dee227c3fd34cdd94674a2643aef397/handler/xmodule_handler" data-next-url="/courses/course-v1:IRONHACK+DAFT+201906_MIA/courseware/26bdd799b69648c083b612c305cae17b/aac9162564384e31bf24c402d1064692/?child=first" data-prev-url="/courses/course-v1:IRONHACK+DAFT+201906_MIA/courseware/26bdd799b69648c083b612c305cae17b/f37806c1af8440b1aed9582145f039db/?child=last">

  <div class="sequence-nav ih-js-fire-highlighting-controls">
    <button class="sequence-nav-button button-previous">
      <span class="icon fa fa-chevron-prev" aria-hidden="true"></span>
      <span class="sequence-nav-button-label">Previous</span>
    </button>
    <button class="sequence-nav-button button-next">
      <span class="sequence-nav-button-label">Next</span>
      <span class="icon fa fa-chevron-next" aria-hidden="true"></span>
    </button>
    <nav class="sequence-list-wrapper" aria-label="Sequence">
      <ol id="sequence-list" role="tablist">
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="0"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@cbf5f55c511349cb9697084f1b2cbdf0"
            data-element="1"
            data-page-title="Introduction to Probability"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Introduction to Probability"
            data-graded="False"
            id="tab_0"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Introduction to Probability<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="1"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@7c7ed7c548e143b4b7b418f148e9e0b1"
            data-element="2"
            data-page-title="Continuous Distributions"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Continuous Distributions"
            data-graded="False"
            id="tab_1"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Continuous Distributions<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="2"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@71f7c91b9d5b46ebad5801589cb7170a"
            data-element="3"
            data-page-title="Discrete Probability Distributions"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Discrete Probability Distributions"
            data-graded="False"
            id="tab_2"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Discrete Probability Distributions<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="3"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@193c97423526429ebc5346036e9148ef"
            data-element="4"
            data-page-title="Calculating Odds"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Calculating Odds"
            data-graded="False"
            id="tab_3"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Calculating Odds<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="4"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@3ee84b3820a843478743111fcb5364f0"
            data-element="5"
            data-page-title="Hypothesis Testing and Statistical Significance"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Hypothesis Testing and Statistical Significance"
            data-graded="False"
            id="tab_4"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Hypothesis Testing and Statistical Significance<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="5"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@8c498edf120c4361b348973fe5d8f0b4"
            data-element="6"
            data-page-title="Two Sample Hypothesis Tests with Scipy"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Two Sample Hypothesis Tests with Scipy"
            data-graded="False"
            id="tab_5"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Two Sample Hypothesis Tests with Scipy<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="6"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@798f359ad2524624879cde57e5129377"
            data-element="7"
            data-page-title="More Hypothesis Testing with SciPy and Stats Models"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; More Hypothesis Testing with SciPy and Stats Models"
            data-graded="False"
            id="tab_6"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>More Hypothesis Testing with SciPy and Stats Models<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="7"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@135c4ad869664502a9b5ceadbb524938"
            data-element="8"
            data-page-title="Regression"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Regression"
            data-graded="False"
            id="tab_7"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Regression<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="8"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@a9cecb6aedca4e24aab92242dfff6300"
            data-element="9"
            data-page-title="Introduction to Bayesian Statistics"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Introduction to Bayesian Statistics"
            data-graded="False"
            id="tab_8"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Introduction to Bayesian Statistics<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
        <li role="presentation">
          <button class="seq_other inactive nav-item tab"
            role="tab"
            tabindex="-1"
            aria-selected="false"
            aria-expanded="false"
            aria-controls="seq_content"
            data-index="9"
            data-id="block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@d0efa5d38bbf43db927f3753ce86aa8c"
            data-element="10"
            data-page-title="Principal Component Analysis"
            data-path="Module 2 &gt; Inferential Statistics in Python &gt; Principal Component Analysis"
            data-graded="False"
            id="tab_9"
            >
            <span class="icon fa seq_other" aria-hidden="true"></span>
              <span 
                class="fa fa-check-circle check-circle is-hidden" 
                style="color:green"
                aria-hidden="true"
              ></span>
            <span class="fa fa-fw fa-bookmark bookmark-icon is-hidden" aria-hidden="true"></span>
            <div class="sequence-tooltip sr"><span class="sr">other&nbsp;</span>Principal Component Analysis<span class="sr bookmark-icon-sr">&nbsp;</span></div>
          </button>
        </li>
      </ol>
    </nav>
  </div>
  <div class="sr-is-focusable" tabindex="-1"></div>

  <div id="seq_contents_0"
    aria-labelledby="tab_0"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@cbf5f55c511349cb9697084f1b2cbdf0&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Introduction to Probability&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@cbf5f55c511349cb9697084f1b2cbdf0&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@526ed25316a04cb58f9a66acdf79306a&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@526ed25316a04cb58f9a66acdf79306a&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;&lt;a id=&#34;user-content-lesson-goals&#34; class=&#34;anchor&#34; href=&#34;#lesson-goals&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Lesson Goals&lt;/h2&gt;
&lt;p&gt;In this lesson we will cover the fundamentals of probability.&lt;/p&gt;
&lt;h2&gt;&lt;a id=&#34;user-content-introduction&#34; class=&#34;anchor&#34; href=&#34;#introduction&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Probability theory was developed in the seventeenth century to gain a better understanding of games of chance. Today these theories are the basis for data science and have many other real world applications. We will learn about the fundamentals of probability theory and apply what we have learned using Python.&lt;/p&gt;
&lt;h2&gt;&lt;a id=&#34;user-content-key-concepts&#34; class=&#34;anchor&#34; href=&#34;#key-concepts&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Key Concepts&lt;/h2&gt;
&lt;p&gt;In order to learn about probability, we must first speak the language of probability.&lt;/p&gt;
&lt;h3&gt;&lt;a id=&#34;user-content-sample-space&#34; class=&#34;anchor&#34; href=&#34;#sample-space&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Sample Space&lt;/h3&gt;
&lt;p&gt;Since probability is rooted in games of chance, we will use different games of chance to explain probability. When playing a game of chance, we consider the game to be an experiment with a finite number of outcomes. The list of all those possible outcomes is called the sample space.&lt;/p&gt;
&lt;p&gt;For example, if we toss a coin, all possible outcomes are heads and tails. If we roll a die, all possible outcomes are 1, 2, 3, 4, 5, 6.&lt;/p&gt;
&lt;p&gt;We use the Greek letter Omega Ω to denote the entire sample space of a random experiment. For the coin, we typically denote the sample space as Ω = {H, T}. For the die, we denote the sample space as Ω = {1, 2, 3, 4, 5, 6}.&lt;/p&gt;
&lt;h3&gt;&lt;a id=&#34;user-content-events&#34; class=&#34;anchor&#34; href=&#34;#events&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Events&lt;/h3&gt;
&lt;p&gt;We can create a subset of of the sample space called an event. We then enumerate all outcomes in the event.&lt;/p&gt;
&lt;p&gt;For example, we can define the event that die roll is even. In this case A = {2, 4, 6}.&lt;/p&gt;
&lt;p&gt;Another example is the event that a die roll is less than or equal to 2. B = {1, 2}.&lt;/p&gt;
&lt;h3&gt;&lt;a id=&#34;user-content-calculating-probabilities&#34; class=&#34;anchor&#34; href=&#34;#calculating-probabilities&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Calculating Probabilities&lt;/h3&gt;
&lt;p&gt;Calculating probabilities can be seen as the ratio of an event to the entire sample space. We count the number of outcomes in the event and divide them by the number of outcomes in the sample space. We denote a probability with P(Event).&lt;/p&gt;
&lt;p&gt;For example, the probability that a die roll is even is &lt;a href=&#34;https://camo.githubusercontent.com/aafde389991c328fb2a37fa832fa7ce96b490f28/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f70726f622d6f662d6576656e2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/aafde389991c328fb2a37fa832fa7ce96b490f28/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f70726f622d6f662d6576656e2e706e67&#34; alt=&#34;prob of even&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/prob-of-even.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The probability that a die roll is less than or equal to 2 is &lt;a href=&#34;https://camo.githubusercontent.com/84b4b542bad4ea8c5edf83e6fc12c0dab0c1b813/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f70726f622d6c6573732d322e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/84b4b542bad4ea8c5edf83e6fc12c0dab0c1b813/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f70726f622d6c6573732d322e706e67&#34; alt=&#34;prob of two or less&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/prob-less-2.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The probability that we flip a coin and get heads = P(H) = 1/2.&lt;/p&gt;
&lt;p&gt;Let&#39;s try this out in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def even(x):
    return(x % 2 == 0)
sample_space = [1, 2, 3, 4, 5, 6]
die_sides = len(sample_space)
even_roll = len([x for x in sample_space if even(x)])
even_probability = even_roll / die_sides
print(even_probability)
0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id=&#34;user-content-union-and-intersection&#34; class=&#34;anchor&#34; href=&#34;#union-and-intersection&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Union and Intersection&lt;/h3&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-union&#34; class=&#34;anchor&#34; href=&#34;#union&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Union&lt;/h4&gt;
&lt;p&gt;The union of sets A and B is denoted by A &lt;span id=&#34;docs-internal-guid-23131c86-7fff-fb06-1574-3813e8a3b809&#34;&gt;U&lt;/span&gt;&lt;span id=&#34;docs-internal-guid-23131c86-7fff-fb06-1574-3813e8a3b809&#34;&gt;&lt;span id=&#34;docs-internal-guid-7f59d9ad-7fff-c7ca-8a88-d3781544a939&#34;&gt;&lt;/span&gt; &lt;/span&gt;&lt;span id=&#34;docs-internal-guid-23131c86-7fff-fb06-1574-3813e8a3b809&#34;&gt;&lt;/span&gt;B and contains the distinct set of all elements that are in A and all elements that are in B.&lt;/p&gt;
&lt;p&gt;For example, if event A is all odd rolls of a die and event B is all outcomes greater than 4, then their union is:&lt;/p&gt;
&lt;p&gt;A = {1, 3, 5}, B = {5, 6}, A U B = {1, 3, 5, 6}&lt;/p&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-intersection&#34; class=&#34;anchor&#34; href=&#34;#intersection&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Intersection&lt;/h4&gt;
&lt;p&gt;The intersection of sets A and B is denoted by A &amp;cap; B and contains all elements that are both in A and in B.&lt;/p&gt;
&lt;p&gt;For example, using our previously defined events A and B, the intersection of A and B is:&lt;/p&gt;
&lt;p&gt;A &amp;cap; B = {5}&lt;/p&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-python-example&#34; class=&#34;anchor&#34; href=&#34;#python-example&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Python Example&lt;/h4&gt;
&lt;p&gt;Recall that we have learned about the set data structure in previous lessons. We can create two sets and find the intersection and the union of those sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;a = set([1, 3, 5])
b = set([5, 6])
a.union(b)
{1, 3, 5, 6}
a.intersection(b)
{5}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id=&#34;user-content-the-three-axioms-of-probability&#34; class=&#34;anchor&#34; href=&#34;#the-three-axioms-of-probability&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;The Three Axioms of Probability&lt;/h2&gt;
&lt;p&gt;Now that we have defined key concepts, we can look at the three rules that define the entire field of probability:&lt;/p&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-axiom-1&#34; class=&#34;anchor&#34; href=&#34;#axiom-1&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Axiom 1&lt;/h4&gt;
&lt;p&gt;The probability of an event is a non negative real number.&lt;/p&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-axiom-2&#34; class=&#34;anchor&#34; href=&#34;#axiom-2&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Axiom 2&lt;/h4&gt;
&lt;p&gt;The probability of the entire sample space is 1 or P(Ω) = 1.&lt;/p&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-axiom-3&#34; class=&#34;anchor&#34; href=&#34;#axiom-3&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Axiom 3&lt;/h4&gt;
&lt;p&gt;The union of mutually exclusive events is equal to the sum of these events. In mathematical notation:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/4356720825bc269ad8049ee3e0fd6f889b5d01c4/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6178696f6d332e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/4356720825bc269ad8049ee3e0fd6f889b5d01c4/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6178696f6d332e706e67&#34; alt=&#34;axiom 3&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/axiom3.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id=&#34;user-content-some-probability-definitions&#34; class=&#34;anchor&#34; href=&#34;#some-probability-definitions&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Some Probability Definitions&lt;/h2&gt;
&lt;h3&gt;&lt;a id=&#34;user-content-mutual-exclusivity&#34; class=&#34;anchor&#34; href=&#34;#mutual-exclusivity&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Mutual Exclusivity&lt;/h3&gt;
&lt;p&gt;Events that are mutually exclusive are events that have no intersection. In probability we say that their intersection is the empty set.&lt;/p&gt;
&lt;p&gt;For example, all odd die rolls and all even die rolls are mutually exclusive events.&lt;/p&gt;
&lt;h3&gt;&lt;a id=&#34;user-content-conditional-probability&#34; class=&#34;anchor&#34; href=&#34;#conditional-probability&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Conditional Probability&lt;/h3&gt;
&lt;p&gt;The conditional probability P(A|B) is the probability of event A given that event B as occurred. We can think of this as limiting the sample space to the event B and then recomputing the probability of A in this new sample space.&lt;/p&gt;
&lt;p&gt;We compute the new probability using the following equation:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/93285fcc2fad9f5735baaedd5a2113f0ee6f5829/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f636f6e646974696f6e616c2d70726f622e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/93285fcc2fad9f5735baaedd5a2113f0ee6f5829/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f636f6e646974696f6e616c2d70726f622e706e67&#34; alt=&#34;conditional prob&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/conditional-prob.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For example, the probability that a die roll is less than 3 given that it is odd is&lt;/p&gt;
&lt;p&gt;less than 3 = {1, 2}&lt;/p&gt;
&lt;p&gt;odd = {1, 3, 5}&lt;/p&gt;
&lt;p&gt;less than 3 &amp;cap; odd = {1}&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/0613a197345882dc016a854212a256d019d308c8/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f636f6e646974696f6e616c2d6578616d706c652e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/0613a197345882dc016a854212a256d019d308c8/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f636f6e646974696f6e616c2d6578616d706c652e706e67&#34; alt=&#34;conditional example&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/conditional-example.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id=&#34;user-content-independence-and-dependence-of-events&#34; class=&#34;anchor&#34; href=&#34;#independence-and-dependence-of-events&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Independence and Dependence of Events&lt;/h3&gt;
&lt;p&gt;Two events are said to be independent if one does not impact the other.&lt;/p&gt;
&lt;p&gt;In order for 2 events A and B to be independent, at least one of the following statements must be true:&lt;/p&gt;
&lt;p&gt;P(A)*P(B) = P(A &amp;cap; B)&lt;/p&gt;
&lt;p&gt;P(A|B) = P(A)&lt;/p&gt;
&lt;p&gt;P(B|A) = P(B)&lt;/p&gt;
&lt;h2&gt;&lt;a id=&#34;user-content-summary&#34; class=&#34;anchor&#34; href=&#34;#summary&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this lesson we learned the fundamentals of probability. These will provide us with a strong foundation for our future endeavors in data analysis.&lt;/p&gt;
&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_1"
    aria-labelledby="tab_1"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@7c7ed7c548e143b4b7b418f148e9e0b1&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Continuous Distributions&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@7c7ed7c548e143b4b7b418f148e9e0b1&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@c0e2777fd8fd4eda9fbb3659bce7050e&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@c0e2777fd8fd4eda9fbb3659bce7050e&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;Lesson Goals&amp;nbsp;&lt;/h2&gt;
&lt;p&gt;In this lesson we are going to learn about the concept of continuous random variables and common continuous distributions including&amp;nbsp;Uniform Distribution, Normal Distribution, and Exponential Distribution. We will also provide Python code samples for continuous distribution visualizations.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the field of probability, a distribution function is a function that maps numerical values to probabilities. Typically, all outcomes in the sample space will have a probability associated with them. There are two types of probability distribution functions - continuous and discrete. In this lesson we will focus on&amp;nbsp;continuous distributions.&lt;/p&gt;
&lt;p&gt;Continuous&amp;nbsp;probability is distributed in a range.&amp;nbsp;To calculate the probability, we compute the total area under the distribution curve of the sample space. At a given point of the curve, the area is approximately zero. It should be noted that we can convert a continuous distribution to discrete distribution by converting the smooth curve into multiple &lt;em&gt;bins&lt;/em&gt; where each bin represents the probability value of a certain range. See image below for example:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;img width=&#34;720&#34; src=&#34;/assets/courseware/v1/20ac1062a000f1e81b2beacb69119470/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/continuous-vs-discrete-distributions.png&#34; alt=&#34;Continuous vs Discrete Distributions&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;Continuous Random Variables&lt;/h3&gt;
&lt;p&gt;Continuous distributions come from continuous random variables. For these random variables, the set of possible values is uncountable. They are defined by probability density functions (PDF).&amp;nbsp;We say X is a continuous random variable if there exists a non-negative function f (x) defined for all x &amp;isin; B, where B is a set of real numbers&amp;nbsp;in &amp;nbsp;(&amp;minus;&amp;infin;, &amp;infin;).&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;40&#34; width=&#34;159&#34; src=&#34;/assets/courseware/v1/8fd9e241aed0b2a58b3e3bf576848df9/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.22.32_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To explain a little deeper, let B&amp;nbsp;be defined&amp;nbsp;as an interval &amp;nbsp;[a, b]:&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;49&#34; width=&#34;171&#34; src=&#34;/assets/courseware/v1/bc6d057a84b43d081284c02fde5132fd/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.24.09_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we let a = b in the preceding formula, then:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;50&#34; width=&#34;175&#34; src=&#34;/assets/courseware/v1/7916ea3aa39d23e3c61c52e731e6914d/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.24.53_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In words, this equation states that the probability that a continuous random variable will assume any particular value is zero.&lt;/p&gt;
&lt;h3&gt;Cumulative&amp;nbsp;Distribution Function&lt;/h3&gt;
&lt;p&gt;A cumulative distribution is defined as:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;59&#34; width=&#34;305&#34; src=&#34;/assets/courseware/v1/963cbe4dfcbb1db77827242e1caff2c3/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.28.19_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some important properties of CDF are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is a continuously increasing function (non-decreasing)&lt;/li&gt;
&lt;li&gt;Range of values lies between [0,1]&lt;/li&gt;
&lt;li&gt;Differentiating CDF gives PDF&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Common Continuous Distributions&lt;/h2&gt;
&lt;p&gt;Now&amp;nbsp;we will discuss some common types of continuous distributions.&lt;/p&gt;
&lt;h2&gt;Uniform Distribution&lt;/h2&gt;
&lt;p&gt;Uniform distribution is defined by the following function:&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;62&#34; width=&#34;252&#34; src=&#34;/assets/courseware/v1/32fb8b6033b0608ebf800b115bff2755/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.30.26_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A more general definition is given as follows where x is not just restricted between 0 and 1:&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;83&#34; width=&#34;269&#34; src=&#34;/assets/courseware/v1/7fd276c25d2a78e16e1cf38eaf59fa35/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.31.30_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: The range of values on which the function is defined is called the domain whereas the values of the function mapped to the domain is called range.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Domain for x is&amp;nbsp; (-&amp;infin;, &amp;infin;)&lt;/li&gt;
&lt;li&gt;Range is 0, 1&lt;/li&gt;
&lt;li&gt;Parameters: a, b&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img height=&#34;292&#34; width=&#34;207&#34; src=&#34;/assets/courseware/v1/0b12487cec4b9aa2945791897701712f/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.32.51_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One of the most&amp;nbsp;common applications of uniform numbers is&amp;nbsp;to generate random numbers without any bias. The probability of obtaining an unbiased random number in the range (a,b) is always constant.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE: The uniform distribution can also be discrete. Consider the example of rolling a die. Probability of each event happening is the same (1/6).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Standard Uniform Distribution&amp;nbsp;in&amp;nbsp;Python code example:&lt;/p&gt;
&lt;pre&gt;import numpy as np
from scipy.stats import uniform
import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 1)
x = np.linspace(0,1, 100)
ax.plot(x, uniform.pdf(x))&lt;/pre&gt;
&lt;p&gt;&lt;img width=&#34;264&#34; src=&#34;/assets/courseware/v1/25448e4ed84ac232cdca7f403b6aa20d/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Uniform_Distribution.png&#34; alt=&#34;Uniform Distribution Plot from python code&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Uniform Distribution between point (a,b):&lt;/p&gt;
&lt;pre&gt;fig, ax = plt.subplots(1, 1)
a=1
b=3
x = np.linspace(a,b, 100)
y = uniform.pdf(x, a, b)
ax.plot(x, y)&lt;/pre&gt;
&lt;p&gt;&lt;img width=&#34;272&#34; src=&#34;/assets/courseware/v1/70e3724181202f33bbe5ad738c439e19/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Uniform_Distribution__a_b_.png&#34; alt=&#34;Non standard uniform distribution&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;Normal Distribution&lt;/h2&gt;
&lt;p&gt;Normal Distribution is also known as the &lt;i&gt;bell shaped curve&lt;/i&gt;. The&amp;nbsp;distribution function is defined as:&lt;/p&gt;
&lt;p&gt;&lt;img width=&#34;351&#34; src=&#34;/assets/courseware/v1/2d97410904b7730406059819350aaeb8/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.36.12_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Domain: Any real number&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Range: Any real number&lt;/li&gt;
&lt;li&gt;Parameters: mean, sd&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img width=&#34;407&#34; src=&#34;/assets/courseware/v1/98e2846b82edea6b858696aea3efc3ed/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.34.38_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some important properties of normal distribution are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is symmetrical about the mean&lt;/li&gt;
&lt;li&gt;Total area under the curve is 1&lt;/li&gt;
&lt;li&gt;The parameters &amp;nbsp;and determine the shape of the curve&lt;/li&gt;
&lt;li&gt;Mean and standard deviation can take on any real number&lt;/li&gt;
&lt;li&gt;When =0 and =1, it is called a Standard Normal Deviation&lt;/li&gt;
&lt;li&gt;&amp;lsquo;x&amp;rsquo; is referred to a &amp;lsquo;z&amp;rsquo; / standardized x&lt;/li&gt;
&lt;li&gt;In a standard normal table has area to the left of z ie CDF value&lt;/li&gt;
&lt;li&gt;The area under the standard normal curve to the left of z-score gives the P( z &amp;lt; z-score)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python code example:&lt;/p&gt;
&lt;pre&gt;import math
from scipy.stats import norm
normal = norm(0, math.sqrt(9)) 
print(normal.pdf(4))
print(normal.cdf(2))       
print(normal.rvs()) 
fig, ax = plt.subplots(1, 1)
x = np.linspace(-3,3,1000)
y = norm.pdf(x)
ax.plot(x,y)
plt.show()&lt;/pre&gt;
&lt;p&gt;&lt;img height=&#34;310&#34; width=&#34;310&#34; src=&#34;/assets/courseware/v1/f2c6340f5ef9211c96dc515ebe0d110b/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Normal_Distribution.png&#34; alt=&#34;Normal distribution from python code&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;Exponential Distribution&lt;/h3&gt;
&lt;p&gt;Exponential Distribution function is defined as:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;66&#34; width=&#34;222&#34; src=&#34;/assets/courseware/v1/73ae5558720531ae3279f70c765a45a4/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.39.05_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Domain: Any real number&lt;/li&gt;
&lt;li&gt;Range: Non Negative real numbers&lt;/li&gt;
&lt;li&gt;Parameter: lambda&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img height=&#34;332&#34; width=&#34;338&#34; src=&#34;/assets/courseware/v1/295b895658652241930849c45a5ceff4/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.40.53_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;CDF is given as:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;52&#34; width=&#34;345&#34; src=&#34;/assets/courseware/v1/e20190519f14670074b2cc7f303b5d7e/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_3.41.42_PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The exponential distribution is often concerned with the amount of time until some specific event occurs. For instance, the amount of time customers spend in a store, arrival of buses at the station, the amount of time customers spend on a website, etc. The distributions of these time measures are often exponential.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Python code example:&lt;/p&gt;
&lt;pre&gt;from scipy.stats import expon
exp = expon(1) 
print(exp.pdf(4))
print(exp.cdf(2))       
print(exp.rvs()) 
x = np.linspace(0,10,100)
y = expon.pdf(x)
fig, ax = plt.subplots(1, 1)
ax.plot(x,y)&lt;/pre&gt;
&lt;p&gt;&lt;img height=&#34;259&#34; width=&#34;259&#34; src=&#34;/assets/courseware/v1/e4fb0731533cbe9c377909eb68bbda72/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Exponential_Distribution.png&#34; alt=&#34;Exponential Distribution from Python code&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this lesson we learned about continuous random variable and continuous probability distributions including continuous uniform, normal, and exponential distribution. We learn how to characterize distributions by their PDF (probability density function), CDF (cumulative distribution function) and its parameters. We also looked at how to generate this random variables from a distribution and calculate some statistics using SciPy.&lt;/p&gt;
&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_2"
    aria-labelledby="tab_2"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@71f7c91b9d5b46ebad5801589cb7170a&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Discrete Probability Distributions&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@71f7c91b9d5b46ebad5801589cb7170a&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@65a3ea6c1b9549f3a311dae65aca7915&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@65a3ea6c1b9549f3a311dae65aca7915&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;Lesson Goals&lt;/h2&gt;
&lt;p&gt;In this lesson we will learn about the concepts of discrete distributions along with some different kinds. We will also look at the python library SciPy to implement discrete distributions&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the previous lesson we discussed continuous distributions. In this lesson we will focus on discrete probability distributions. First let&#39;s have a look at the&amp;nbsp;basics.&lt;/p&gt;
&lt;h3&gt;Random Variables&lt;/h3&gt;
&lt;p&gt;Consider an experiment where we are rolling a die twice.&lt;/p&gt;
&lt;pre&gt;S = { (1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (2,1), (2,2), (2,3), .......... }&lt;/pre&gt;
&lt;p&gt;This can be represented by a random variable X as:&lt;/p&gt;
&lt;pre&gt;X = {Sum of numbers on the die when rolled twice}&lt;/pre&gt;
&lt;p&gt;The value of&amp;nbsp;X can fall anywhere between 2 to 12.&lt;/p&gt;
&lt;pre&gt;P{X = 2} = P{(1, 1)} = 1/36&lt;/pre&gt;
&lt;pre&gt;P{X = 3} = P{(1, 2), (2, 1)} =2/36&lt;/pre&gt;
&lt;pre&gt;P{X = 4} = P{(1, 3), (2, 2), (3, 1)} = 3/36&lt;/pre&gt;
&lt;pre&gt;P{X = 5} = P{(1, 4), (2, 3), (3, 2), (4, 1)} = 4/36&lt;/pre&gt;
&lt;pre&gt;P{X = 6} = P{(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)} = 5/36&lt;/pre&gt;
&lt;pre&gt;P{X = 7} = P{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)} = 6/36&lt;/pre&gt;
&lt;pre&gt;P{X = 8} = P{(2, 6), (3, 5), (4, 4), (5, 3), (6, 2)} = 5/36&lt;/pre&gt;
&lt;pre&gt;P{X = 9} = P{(3, 6), (4, 5), (5, 4), (6, 3)} = 4/36&lt;/pre&gt;
&lt;pre&gt;P{X = 10} = P{(4, 6), (5, 5), (6, 4)} = 3/36&lt;/pre&gt;
&lt;pre&gt;P{X = 11} = P{(5, 6), (6, 5)} = 2/36&lt;/pre&gt;
&lt;pre&gt;P{X = 12} = P{(6, 6)} = 1/36&lt;br /&gt;&lt;br /&gt;The sum of all the probabilities added together is 1:&lt;br /&gt;&lt;br /&gt; &lt;img width=&#34;119&#34; src=&#34;/assets/courseware/v1/9255b87afc6eb8a30b8551d2e34bfba5/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Discrete_distribution_sum.png&#34; alt=&#34;Summation of all the probabilities in the same space equals 1 &#34; /&gt;&lt;/pre&gt;
&lt;p&gt;Distribution function or cumulative distribution function (CDF): It is a function defined on the random variable X such that X &amp;isin; Real Number &amp;lsquo;a&amp;rsquo; (- inf, inf )&lt;/p&gt;
&lt;pre&gt;F(a) = P{X &amp;le; a}&lt;/pre&gt;
&lt;p&gt;F(a) denotes the probability that the random variable X takes on a value that is less than or equal to &amp;lsquo;a&amp;rsquo;. Some properties of the&amp;nbsp;CDF F are:&lt;/p&gt;
&lt;pre&gt;(i) F(b) is a nondecreasing function of b,&lt;/pre&gt;
&lt;pre&gt;(ii) lim b&amp;rarr;&amp;infin; F(b) = F(&amp;infin;) = 1,&lt;/pre&gt;
&lt;pre&gt;(iii) lim b&amp;rarr;&amp;minus;&amp;infin; F(b) = F(&amp;minus;&amp;infin;) = 0.&lt;/pre&gt;
&lt;p&gt;Points ii and iii essentially say that &amp;nbsp;the value of CDF can lie only in the interval [0,1]&lt;/p&gt;
&lt;h3&gt;Discrete Random Variable&lt;/h3&gt;
&lt;p&gt;A discrete variable can&amp;nbsp;take on either a finite or countable number of possible values. In the example discussed above, the total possible values of X is 11. Therefore, X is a discrete random variable.&amp;nbsp;We define the probability mass function PMF as p(a) of X by:&lt;/p&gt;
&lt;pre&gt;p(a) = P{X = a}&lt;/pre&gt;
&lt;pre&gt;p(xi) &amp;gt; 0, i = 1, 2, . . .&lt;/pre&gt;
&lt;pre&gt;p(x) = 0, all other values of x&lt;/pre&gt;
&lt;p&gt;The cumulative distribution function F can be expressed in terms of p(a) by&lt;/p&gt;
&lt;p&gt;&lt;img height=&#34;50&#34; width=&#34;256&#34; src=&#34;/assets/courseware/v1/a5a3cdf78678c654d8646b9686c527a5/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_12.21.51_PM.png&#34; alt=&#34;cumulative distribution function for discrete distributions&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Distributions arise from random variables.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;h2&gt;Discrete Probability Distributions&lt;/h2&gt;
&lt;h3&gt;Bernoulli&#39;s Random Variable&lt;/h3&gt;
&lt;p&gt;When in an experiment, the outcome can be classified as either a &amp;ldquo;success&amp;rdquo; or as a &amp;ldquo;failure&amp;rdquo;. Therefore the random variable X equal 1 if the outcome is a success and a 0 if it is a failure.&lt;/p&gt;
&lt;pre&gt;Parameter: p&lt;/pre&gt;
&lt;pre&gt;p(1) = P{X = 1} = p &amp;nbsp;where p is the probability of success&lt;/pre&gt;
&lt;pre&gt;p(0) = P{X = 0} = 1 &amp;minus; p&lt;/pre&gt;
&lt;p&gt;The following example uses&amp;nbsp;Python Scipy to plot the distribution of Bernoulli&#39;s random variable. Note that the Bernoulli number sequence is dynamically generated with the probabilistic distribution of p(1) = 0.8 and p(0) = 0.2. That&#39;s why the distribution is approximately instead of exactly&lt;span style=&#34;font-size: 1em;&#34;&gt;&amp;nbsp;0.2/0.8.&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;import matplotlib.pyplot as plt
from scipy.stats import bernoulli
p = 0.8
X = bernoulli.rvs(p, size=100)
plt.hist(X, align=&#39;mid&#39;)&lt;/pre&gt;
&lt;pre&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;img height=&#34;242&#34; width=&#34;285&#34; src=&#34;/assets/courseware/v1/cd6034de27e613a020c66b7ab16dbb04/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Bernoulli_s_distribution_plot.png&#34; alt=&#34;Bernoulli plot from python code &#34; /&gt;&lt;/h3&gt;
&lt;h3&gt;Binomial Random Variable&lt;/h3&gt;
&lt;p&gt;In the previous example we had only experiment where we checked if the event was a success or failure. Consider a similar setup but here you are conducting &amp;lsquo;n&amp;rsquo; such independent experiments and recording the results of every experiment.&lt;/p&gt;
&lt;pre&gt;parameters: (n, p)&lt;/pre&gt;
&lt;pre&gt;N: Number of trials/experiments&lt;/pre&gt;
&lt;pre&gt;P: probability of success&lt;/pre&gt;
&lt;p&gt;The probability mass function of a binomial random variable having parameters (n, p) is given by&lt;/p&gt;
&lt;p&gt;&lt;img width=&#34;350&#34; src=&#34;/assets/courseware/v1/48376e51218054ac4a587f8ee8ea6706/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_12.22.52_PM.png&#34; alt=&#34;Probability mass function of a binomial random variable with parameters (n, p)&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And &amp;lsquo; i &amp;lsquo; is the number of successes in all &amp;lsquo;n&amp;rsquo; trials.&lt;/p&gt;
&lt;p&gt;Scipy example:&lt;/p&gt;
&lt;pre&gt;%matplotlib inline
from scipy import stats
from scipy.stats import binom
n=10
p=0.4
binomial = binom(n,p)
print(binomial.pmf(3))
print(binomial.cdf(3))
print(binomial.mean())
print(binomial.var())
print(binomial.std())
print(binomial.rvs(10))
x = np.arange(0,10)
fig, ax = plt.subplots(1, 1)
ax.plot(x, binom.pmf(x, n, p), &#39;bo&#39;)
ax.vlines(x, 0, binom.pmf(x, n, p), colors=&#39;b&#39;, lw=5, alpha=0.5)&lt;br /&gt;&lt;br /&gt;&lt;img height=&#34;267&#34; width=&#34;267&#34; src=&#34;/assets/courseware/v1/1bf5405b0661e0df622a399c76c3d613/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Binomial_Distribution_Plot__10_0.4_.png&#34; alt=&#34;Binomial Distribution plot from python code&#34; /&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note: We can use the function &#34;help(x)&#34; to check all methods defined on x&amp;nbsp;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Poisson Random Variable&lt;/h3&gt;
&lt;p&gt;This is usually used as a counting variable (count number of occurrences of an event within a time frame). For this reason Poisson processes are also known as counting processes.&lt;/p&gt;
&lt;p&gt;It is defined as:&lt;/p&gt;
&lt;p&gt;&lt;img width=&#34;300&#34; src=&#34;/assets/courseware/v1/f8cae8eb5b4a97095b422846db5321f1/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Screen_Shot_2019-05-20_at_12.24.54_PM.png&#34; alt=&#34;Poisson Random Variable&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;Parameter: &lt;span style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif; white-space: normal;&#34;&gt;&amp;lambda;&amp;nbsp;&lt;/span&gt; (rate of the process)&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the example: Imagine that the number of accidents occurring on a highway each day is a Poisson random variable with parameter &amp;lambda; = 3, what is the probability that no accidents occur today&lt;/p&gt;
&lt;p&gt;This is the same as saying P(X=0).&lt;/p&gt;
&lt;p&gt;Scipy example:&lt;/p&gt;
&lt;pre&gt;%matplotlib inline 
from scipy import stats
from scipy.stats import poisson
param = 2
po = stats.poisson(param)
print(po.pmf(param))     
print(po.rvs(10))
x = np.arange(0,6)
fig, ax = plt.subplots(1, 1)
ax.plot(x, poisson.pmf(x, param), &#39;bo&#39;, ms=8, label=&#39;poisson pmf&#39;)
ax.vlines(x, 0, poisson.pmf(x, param), colors=&#39;b&#39;, lw=5, alpha=0.5)&lt;/pre&gt;
&lt;pre&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;img width=&#34;250&#34; src=&#34;/assets/courseware/v1/667d7c965d9a860f4accae18c2f7e66e/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Poisson_Distribution.png&#34; alt=&#34;Poisson distribution from python code&#34; /&gt;&lt;/h2&gt;
&lt;h2&gt;Summary&amp;nbsp;&lt;/h2&gt;
&lt;p&gt;In this lesson we learnt about discrete random variables,&amp;nbsp;discrete probability distributions, &amp;nbsp;and how they are characterized by their parameters and PMF (probability mass function). We also talked about some important distributions including Bernoulli&#39;s, binomial, and Poisson distribution along with its application in python using SciPy library.&amp;nbsp;&lt;/p&gt;
&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_3"
    aria-labelledby="tab_3"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@193c97423526429ebc5346036e9148ef&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Calculating Odds&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@193c97423526429ebc5346036e9148ef&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@afc5c7d8f45847438d8c6f5bb749f7c1&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@afc5c7d8f45847438d8c6f5bb749f7c1&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;&lt;a id=&#34;user-content-introduction&#34; class=&#34;anchor&#34; href=&#34;#introduction&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The field of probability was developed specifically for games of chance. Therefore, it is only natural that we use games of chance in this lesson. To decide whether a bet is worthwhile for us, it is beneficial to calculate the odds for a particular outcome.&lt;/p&gt;
&lt;h2&gt;&lt;a id=&#34;user-content-odds&#34; class=&#34;anchor&#34; href=&#34;#odds&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Odds&lt;/h2&gt;
&lt;p&gt;We recall looking at the probability of an event. The probability of an event is the ratio between the number of times an event occurred and the size of the sample space. However, the odds of an event are the ratio between the probability that an event will occur and the probability that an event will not occur.&lt;/p&gt;
&lt;p&gt;For example, using a fair die, we can define event A as the case where we get a number less than 3 number in one roll.&lt;/p&gt;
&lt;p&gt;A = {1, 2}&lt;/p&gt;
&lt;p&gt;Therefore, P(A) = 2/6 = 1/3&lt;/p&gt;
&lt;p&gt;We use this probability to calculate the odds of A:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/bd21880ca651524ed2b9665644af60a1a99947fa/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6f6464732e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/bd21880ca651524ed2b9665644af60a1a99947fa/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6f6464732e706e67&#34; alt=&#34;odds&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/odds.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The odds in this case are 1/2. We may also say that the odds are 2:1. It means that for every 3 die rolls 1 will result in event A occurring and 2 will not.&lt;/p&gt;
&lt;p&gt;Let&#39;s define a Python function that computes the odds and verify this result.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; def odds(prob):
...     return(prob/(1-prob))

&amp;gt;&amp;gt;&amp;gt; odds(1/3)
0.49999999999999994&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

  &lt;/div&gt;
  &lt;div class=&#34;vert vert-1&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@5263447e904f4c1ea492889e6944a6fe&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@5263447e904f4c1ea492889e6944a6fe&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;Playing Poker with Python&lt;/h2&gt;
&lt;p&gt;We will use the game of poker to learn more about computing odds. The key in playing poker is to know the probabilities or odds of different hands to make an optimal decision.&lt;/p&gt;
&lt;h3&gt;&lt;a id=&#34;user-content-combinations-and-permutations-a-primer&#34; class=&#34;anchor&#34; href=&#34;#combinations-and-permutations-a-primer&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Combinations and Permutations: A Primer&lt;/h3&gt;
&lt;p&gt;In order to talk about poker hands, we must first introduce combinations and permutations. The most efficient way to count the number of possible ways to get a hand is by using combinations and permutations.&lt;/p&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-combinations&#34; class=&#34;anchor&#34; href=&#34;#combinations&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Combinations&lt;/h4&gt;
&lt;p&gt;A combination is a selection of a subset of objects from a larger set. In a combination, the order in which objects were selected does not matter. A combination of k elements from a set of n elements is known as &#34;n choose k&#34; and noted&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/e54dd141d5e2291e279bdd1d2ffb6bfc152223d8/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6e2d63686f6f73652d6b2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/e54dd141d5e2291e279bdd1d2ffb6bfc152223d8/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6e2d63686f6f73652d6b2e706e67&#34; alt=&#34;n choose k&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/n-choose-k.png&#34; /&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Scipy library contains a function for computing combinations called &lt;code&gt;comb&lt;/code&gt;. For example, here we will compute 52 choose 5.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scipy.misc import comb
&amp;gt;&amp;gt;&amp;gt; comb(52, 5)
2598960.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-permutations&#34; class=&#34;anchor&#34; href=&#34;#permutations&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Permutations&lt;/h4&gt;
&lt;p&gt;Permutations differ from combinations in that the order in which objects are selected matters. Permutations are noted&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/be128f21eeb0b06c3e2f9ca0eb0bd93f435e62d8/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7065726d75746174696f6e2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/be128f21eeb0b06c3e2f9ca0eb0bd93f435e62d8/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7065726d75746174696f6e2e706e67&#34; alt=&#34;permutation&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/permutation.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3&gt;Calculating the Odds of Different Poker Hands&lt;/h3&gt;
&lt;h4&gt;&lt;a id=&#34;user-content-royal-flush&#34; class=&#34;anchor&#34; href=&#34;#royal-flush&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Royal Flush&lt;/h4&gt;
&lt;p&gt;The top hand in poker is a royal flush. A royal flush contains A, K, Q, J, and 10 all from the same suit.&lt;/p&gt;
&lt;p&gt;A deck of cards contains 52 cards. In a deck of cards, we have 4 suits where each suit contains 13 cards. Therefore, to have a royal flush, we must select a suit, so there is a 1 of 4 chance that we select a specific suit. Then each suit has only one of the cards in the hand. Therefore, there are a total of 4 possible royal flushes in the deck.&lt;/p&gt;
&lt;p&gt;The probability of a royal flush is 4 divided by the number of all possible hands. We find the number of all possible hands which is 52 choose 5. Therefore, the probability of a royal flush is &lt;a href=&#34;https://camo.githubusercontent.com/e9861d7561bba7f1b98c92f162155eb035aa6d82/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f726f79616c2d666c7573682e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/e9861d7561bba7f1b98c92f162155eb035aa6d82/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f726f79616c2d666c7573682e706e67&#34; alt=&#34;royal flush&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/royal-flush.png&#34; /&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Using Python we can compute the probability and the odds. We will use the odds function that we have previously defined in this lesson.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; royal_flush_prob = 4/comb(52, 5)
&amp;gt;&amp;gt;&amp;gt; royal_flush_prob
1.5390771693292702e-06
&amp;gt;&amp;gt;&amp;gt; royal_flush_odds = odds(royal_flush_prob)
&amp;gt;&amp;gt;&amp;gt; royal_flush_odds
1.5390795380914492e-06
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The odds are 649,739:1. Which means that for every 649,740 hands played, one will be a royal flush and the rest will not.&lt;/p&gt;
&lt;h4&gt;Straight Flush&lt;/h4&gt;
&lt;p&gt;A straight flush is every sequence of 5 consecutive cards from the same suit that is not a royal flush. There are 10 such sequences in total so we must compute the number of sequences and then subtract the number of royal flushes from that number.&lt;/p&gt;
&lt;p&gt;In order to calculate the probability, we first choose a suit and then choose the sequence. There are 4 suits and 10 sequences, and we pick one of each. Therefore, the probability of a straight flush is: &lt;a href=&#34;https://camo.githubusercontent.com/90f63cd03848170672b47bf6657868b37c7253be/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f73747261696768742d666c7573682e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/90f63cd03848170672b47bf6657868b37c7253be/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f73747261696768742d666c7573682e706e67&#34; alt=&#34;straight flush&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/straight-flush.png&#34; /&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; straight_flush_prob = (comb(10,1)*comb(4,1)-comb(4,1))/comb(52,5)
&amp;gt;&amp;gt;&amp;gt; straight_flush_prob
1.3851694523963431e-05
&amp;gt;&amp;gt;&amp;gt; straight_flush_odds = odds(straight_flush_prob)
&amp;gt;&amp;gt;&amp;gt; straight_flush_odds
1.385188639606237e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The odds of a straight flush are 72,192:1.&lt;/p&gt;
&lt;h3&gt;Poker Hands Probability Table&lt;/h3&gt;
&lt;p&gt;We are not going to calculate the odds and probabilities for all the poker hands for you. Instead, we&#39;ll show you the probability table.&amp;nbsp;We purposefully skip the math formulas for &lt;em&gt;three of a kind&lt;/em&gt;, &lt;em&gt;two pair&lt;/em&gt;, &lt;em&gt;one pair&lt;/em&gt;, and &lt;em&gt;no-pair&lt;/em&gt; because we would like you to figure them out yourself, which you will do in the lab exercise.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Cards&lt;/th&gt;&lt;th&gt;Hand&lt;/th&gt;&lt;th&gt;Distinct hands&lt;/th&gt;&lt;th&gt;Frequency&lt;/th&gt;&lt;th&gt;Probability&lt;/th&gt;&lt;th&gt;Formula&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/f132e406ef71366d966a0d42d6ef01c6/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/royal-flush.jpeg&#34; alt=&#34;royal flush hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;Royal flush&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.000154%&lt;/td&gt;
&lt;td&gt;&lt;img width=&#34;43&#34; src=&#34;/assets/courseware/v1/891d9305a30d31ba2e3fca26bd2e9da6/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/royal-flush.svg&#34; alt=&#34;royal flush formula&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/992465e0ba88dcd49d09d068a59e5eeb/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/straight-flush.jpeg&#34; alt=&#34;straight flush hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;Straight flush (excluding royal flush)&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;0.00139%&lt;/td&gt;
&lt;td&gt;&lt;img width=&#34;120&#34; src=&#34;/assets/courseware/v1/83efcebfe24813d077adfc41ae54aa16/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/straight-flush.svg&#34; alt=&#34;straight flush formula&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/05cb766083ee4ac1f76daae98d21ad19/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/four-of-kind.jpeg&#34; alt=&#34;four of kind hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;Four of a kind&lt;/td&gt;
&lt;td&gt;156&lt;/td&gt;
&lt;td&gt;624&lt;/td&gt;
&lt;td&gt;0.0240%&lt;/td&gt;
&lt;td&gt;&lt;img width=&#34;112&#34; src=&#34;/assets/courseware/v1/566723f19f3c239ca346ea08609db233/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/four-of-kind.svg&#34; alt=&#34;four of kind formula&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/668148316cb26991e63ee575eec5d8aa/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/full-house.jpeg&#34; alt=&#34;full house hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;Full house&lt;/td&gt;
&lt;td&gt;156&lt;/td&gt;
&lt;td&gt;3,744&lt;/td&gt;
&lt;td&gt;0.1441%&lt;/td&gt;
&lt;td&gt;&lt;img width=&#34;142&#34; src=&#34;/assets/courseware/v1/070b7a9ea3a487bbd5b056b111e97bdf/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/full-house.svg&#34; alt=&#34;full house formula&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/96abb8122ac6c0100d284bdb5d2b4223/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/flush.jpeg&#34; alt=&#34;flush hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;Flush (excluding royal flush and straight flush)&lt;/td&gt;
&lt;td&gt;1,277&lt;/td&gt;
&lt;td&gt;5,108&lt;/td&gt;
&lt;td&gt;0.1965%&lt;/td&gt;
&lt;td&gt;&lt;img width=&#34;159&#34; src=&#34;/assets/courseware/v1/52fa225f98f653e252ec60ca13d5a4ff/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/flush.svg&#34; alt=&#34;flush formula&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/01267f7d6c1b07dee42c80aa82a51f59/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/straight.jpeg&#34; alt=&#34;straight hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;Straight (excluding royal flush and straight flush)&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;10,200&lt;/td&gt;
&lt;td&gt;0.3925%&lt;/td&gt;
&lt;td&gt;&lt;img width=&#34;165&#34; src=&#34;/assets/courseware/v1/99af4d1ffbb95dc88ed8d0c188f33ad3/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/straight.svg&#34; alt=&#34;straight formula&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/edba71c647782e6eb8e17c1982e9855a/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/three-of-kind.jpeg&#34; alt=&#34;three of kind hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;Three of a kind&lt;/td&gt;
&lt;td&gt;858&lt;/td&gt;
&lt;td&gt;54,912&lt;/td&gt;
&lt;td&gt;2.1128%&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/ae6577cdd0c2f8e5d3c6bb10183efaec/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/two-pair.jpeg&#34; alt=&#34;two pair hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;Two pair&lt;/td&gt;
&lt;td&gt;858&lt;/td&gt;
&lt;td&gt;123,552&lt;/td&gt;
&lt;td&gt;4.7539%&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/b07d4f77d62ab4cb03085c155584f482/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/no-pair.jpeg&#34; alt=&#34;one pair hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;One pair&lt;/td&gt;
&lt;td&gt;2,860&lt;/td&gt;
&lt;td&gt;1,098,240&lt;/td&gt;
&lt;td&gt;42.2569%&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img width=&#34;200&#34; src=&#34;/assets/courseware/v1/b07d4f77d62ab4cb03085c155584f482/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/no-pair.jpeg&#34; alt=&#34;no pair hand&#34; /&gt;&lt;/td&gt;
&lt;td&gt;No pair&lt;/td&gt;
&lt;td&gt;1,277&lt;/td&gt;
&lt;td&gt;1,302,540&lt;/td&gt;
&lt;td&gt;50.1177%&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;

  &lt;/div&gt;
  &lt;div class=&#34;vert vert-2&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@9266a94d660a403aacc8b3792cb6daf9&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@9266a94d660a403aacc8b3792cb6daf9&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this lesson we learned how to compute probabilities and odds for a couple of different poker hands. To do this, we learned about combinations and permutations.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_4"
    aria-labelledby="tab_4"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@3ee84b3820a843478743111fcb5364f0&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Hypothesis Testing and Statistical Significance&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@3ee84b3820a843478743111fcb5364f0&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@10918f857d284ee4b4e05ce56617c9b9&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@10918f857d284ee4b4e05ce56617c9b9&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;Lesson Goals&lt;/h2&gt;
&lt;p&gt;In this lesson we will learn the fundamentals of hypothesis testing and statistical significance.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many times in the real world we would like to resolve a question that requires a comparison of two quantities. For example, does leaving the light on cause people to take longer to fall asleep, or do books with more pages sell more copies. Using a few basic assumptions, we can use statistical inference to come to a conclusion and determine an answer for these questions. Hypothesis allow us to compare two samples and using certain assumptions we can either reject or not reject our hypothesis (as true statisticians we never say that we accept a hypothesis only reject or do not reject).&lt;/p&gt;
&lt;h2&gt;&lt;a id=&#34;user-content-key-concepts&#34; class=&#34;anchor&#34; href=&#34;#key-concepts&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Key Concepts&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Population&lt;/strong&gt; - In Statistics,&amp;nbsp;a &lt;em&gt;population&lt;/em&gt; is a set of similar items or events which is of interest for some question or experiment. For example, say we&amp;nbsp;want to analyze the&amp;nbsp;income data of all&amp;nbsp;residents in New York City. In this case the population is all the NYC residents.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample&lt;/strong&gt; - A&amp;nbsp;&lt;em&gt;sample&lt;/em&gt; is a subset&amp;nbsp;of the population chosen to represent the population in statistical analysis. For example, in reality it is impossible to obtain all residents&#39; income&amp;nbsp;information in NYC because there are too many people. In order to analyze NYC residents&#39; income&amp;nbsp;data, we&amp;nbsp;can&amp;nbsp;select a decent number of residents&amp;nbsp;and use their income&amp;nbsp;information to represent&amp;nbsp;all NYC residents. Ideally, the selection of the sample should be randomized in order to avoid&amp;nbsp;&lt;em&gt;sampling&amp;nbsp;bias&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sampling Bias&lt;/strong&gt; - &lt;em&gt;Sampling bias&lt;/em&gt; occurs when&amp;nbsp;some members of the intended population are less likely to be included than others in the selected sample. Say we collected the data&amp;nbsp;at Wall Street at 10AM on Monday. Significant bias can&amp;nbsp;be introduced due to the flaws of the data collection because people who appear at Wall Street at that specific time are most likely working professionals in the financial industry. The sample data will miss a lot of people&amp;nbsp;from other industries and other NYC neighborhoods. Consequently, the selected sample is not a good representation of NYC residents.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistical&amp;nbsp;Hypothesis&lt;/strong&gt; -&amp;nbsp;A &lt;em&gt;statistical hypothesis&lt;/em&gt; is an assumption about the population average. For example, we can assume the average income of NYC residents is $850,000.&amp;nbsp;&lt;span style=&#34;font-size: 1em;&#34;&gt;This assumption may or may not be true.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis&lt;/strong&gt; - Denoted&amp;nbsp;with&amp;nbsp;&lt;strong&gt;H&lt;sub&gt;0&lt;/sub&gt;&lt;/strong&gt;&lt;span style=&#34;font-size: 1em;&#34;&gt;, a&amp;nbsp;&lt;/span&gt;&lt;em style=&#34;font-size: 1em;&#34;&gt;null hypothesis&lt;/em&gt;&lt;span style=&#34;font-size: 1em;&#34;&gt; is an assumption that the population average is identical to a specific value.&amp;nbsp;&lt;/span&gt;&lt;span style=&#34;font-size: 1em;&#34;&gt;&amp;nbsp;The typical notation is&amp;nbsp;&lt;/span&gt;&lt;em&gt;&lt;span style=&#34;font-family: monospace, serif; white-space: pre-wrap;&#34;&gt;&amp;mu; = &lt;/span&gt;&lt;span style=&#34;font-size: 1em; font-family: monospace, serif; white-space: pre-wrap;&#34;&gt;&amp;mu;&lt;/span&gt;&lt;/em&gt;&lt;span style=&#34;font-size: 1em; font-family: monospace, serif; white-space: pre-wrap; vertical-align: sub;&#34;&gt;&lt;em&gt;0&lt;/em&gt;&lt;/span&gt;&lt;span style=&#34;font-family: monospace, serif; white-space: pre-wrap;&#34;&gt;, where &lt;/span&gt;&lt;span style=&#34;font-family: monospace, serif; white-space: pre-wrap;&#34;&gt;&amp;mu; &lt;/span&gt;&lt;span style=&#34;font-family: monospace, serif; white-space: pre-wrap; font-size: 1em;&#34;&gt;refers to the population mean and &lt;/span&gt;&lt;em&gt;&lt;span style=&#34;font-size: 1em; font-family: monospace, serif; white-space: pre-wrap;&#34;&gt;&amp;mu;&lt;/span&gt;&lt;/em&gt;&lt;span style=&#34;font-size: 1em; font-family: monospace, serif; white-space: pre-wrap; vertical-align: sub;&#34;&gt;&lt;em&gt;0 &lt;/em&gt;&lt;/span&gt;&lt;span style=&#34;font-family: monospace, serif; font-size: 1em; white-space: pre-wrap;&#34;&gt;refers to the hypothesized value. &lt;/span&gt;For example, a null hypothesis can be that the average NYC residents&#39; income is $850,000. The null hypothesis is represented by:&lt;/p&gt;
&lt;pre&gt;H&lt;sub&gt;0&lt;/sub&gt;: &amp;mu; = 850,000&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Alternative Hypothesis&lt;/strong&gt;&amp;nbsp;- An &lt;em&gt;alternative hypothesis&lt;/em&gt; is&amp;nbsp;the opposite of the null hypothesis. We compare this hypothesis with the null hypothesis to decide whether or not we reject the null hypothesis. We denote the alternative hypothesis with&lt;strong&gt; H&lt;sub&gt;1&lt;/sub&gt;&lt;/strong&gt; or &lt;strong&gt;H&lt;sub&gt;a&lt;/sub&gt;&lt;/strong&gt;.&amp;nbsp;For the previous example of the null hypothesis, the alternative hypothesis can be any of the following:&lt;/p&gt;
&lt;pre&gt;H&lt;sub&gt;1&lt;/sub&gt;: &amp;mu; &amp;gt; 850,000&lt;/pre&gt;
&lt;pre style=&#34;font-size: 16px;&#34;&gt;H&lt;sub&gt;1&lt;/sub&gt;: &amp;mu; &amp;lt; 850,000&lt;/pre&gt;
&lt;pre style=&#34;font-size: 16px;&#34;&gt;H&lt;sub&gt;1&lt;/sub&gt;: &amp;mu; &amp;ne;&amp;nbsp;850,000&lt;br /&gt;&lt;br /&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id=&#34;user-content-types-of-hypotheses&#34; class=&#34;anchor&#34; href=&#34;#types-of-hypotheses&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Types of Alternative Hypotheses&lt;/h2&gt;
&lt;p&gt;There are&amp;nbsp;two types of alternative hypotheses: &lt;strong&gt;one-sided&lt;/strong&gt; and &lt;strong&gt;two-sided&lt;/strong&gt;. One-sided alternative hypothesis (a.k.a.&amp;nbsp;directional hypothesis) is used to determine whether the population average&amp;nbsp;differs from the hypothesized value in a specific direction (larger but not smaller than, or vice versa). In contrast, two-sided alternative hypothesis (a.k.a. nondirectional hypothesis) is used to determine whether the population average&amp;nbsp;is either greater than or less than the hypothesized value.&lt;/p&gt;
&lt;p&gt;Why we differentiate these two types? Because sometimes we are not interested in whether the population average is larger or smaller than the hypothesized value. We only care if they are different. In this case, we use two-sided instead of one-sided hypothesis. However, in case we care about which is the case, we use one-sided hypothesis.&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;th&gt;Null Hypothesis&lt;/th&gt;&lt;th&gt;One-sided Alternative Hypothesis&lt;/th&gt;&lt;th&gt;Two-sided Alternative Hypothesis&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H&lt;sub style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif;&#34;&gt;0&lt;/sub&gt;&lt;span style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif; font-size: 16px;&#34;&gt;: &amp;mu; = k&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;H&lt;sub style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif;&#34;&gt;1&lt;/sub&gt;&lt;span style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif; font-size: 16px;&#34;&gt;: &amp;mu; &amp;gt; k or&amp;nbsp;&lt;/span&gt;H&lt;sub style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif;&#34;&gt;1&lt;/sub&gt;&lt;span style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif; font-size: 16px;&#34;&gt;: &amp;mu; &amp;lt; k&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;H&lt;sub style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif;&#34;&gt;1&lt;/sub&gt;&lt;span style=&#34;font-family: &#39;Open Sans&#39;, Verdana, Arial, Helvetica, sans-serif; font-size: 16px;&#34;&gt;: &amp;mu; &amp;ne; k&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Test Assumptions&lt;/h2&gt;
&lt;p&gt;In order to conduct a hypothesis test we need to meet certain assumptions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Our observations must be independent of each other. For example, if we have people who live in the same household participating in a medical trial, they might be exposed to the same environmental conditions or eat the same food. This can bias our results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Normality of data - We assume that the sample is derived from a normally distributed data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adequate sample size. In order to perform a test using the normal distribution and not approximate to the t distribution, our sample size must be greater than 30.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In order to use the normal distribution for our hypothesis test, we must assume the population standard deviation is known. If the population standard deviation is not known, then we use the t-distribution for the hypothesis test.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Test Statistic&lt;/h2&gt;
&lt;p&gt;Once we determine the type of hypothesis test and that our assumptions have been met, we use our data to decide whether to reject or not reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;For the z test:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/db5447d99a80876c820864e10f6b36f05fe237af/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6e6f726d616c2d746573742e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/db5447d99a80876c820864e10f6b36f05fe237af/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6e6f726d616c2d746573742e706e67&#34; alt=&#34;z test&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/normal-test.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For the t test:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/e7611f9e6d5f5ce6f1bb6ab4a4639936f0385ce2/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f742d746573742e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/e7611f9e6d5f5ce6f1bb6ab4a4639936f0385ce2/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f742d746573742e706e67&#34; alt=&#34;t test&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/t-test.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This means that our test statistic is equal to the difference between the sample mean and our constant divided by the standard deviation over the square root of the sample size.&lt;/p&gt;
&lt;p&gt;We compare the z or t value found in the test with the boundary that we determine for the test using the significance that we chose for the test and determine the result of our test.&lt;/p&gt;
&lt;h2&gt;Test significance&lt;/h2&gt;
&lt;p&gt;Typically when we perform a hypothesis test, we cannot be 100% sure of the result. For example, if our null hypothesis is the NYC residents&#39; average income is $85,000 but the&amp;nbsp;observed value is $85,001, should we reject the null hypothesis? Probably not because $85,001 is so close to $85,000. However,&amp;nbsp;what if the observed value is&amp;nbsp;$85,100? How about $86,000?&amp;nbsp;Eventually we need to reject the null hypothesis at a certain point. What is the threshold at which we&amp;nbsp;are confident enough to reject the null hypothesis?&lt;/p&gt;
&lt;p&gt;In statistical analysis, we use test significance to indicate whether we are confident enough to reject the null hypothesis. And the test significance is represented by the probability score we calculate from the z or t test. Typically, we are confident to reject when we are 95% confident by using the 5% significance level (&lt;em&gt;&amp;alpha; = 0.05&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Having 95% confidence means that we start off with the null hypothesis as the status quo. We compute our test statistic given that we think the null hypothesis is true. The significance that we produce from the test statistic is the probability that we obtained our result due to random chance. If there is less than a 5% chance that we obtained our result due to random chance, that is a very small probability of this outcome being random and it is most likely happening due to something in the experiment. Then we can reject the null hypothesis.&lt;/p&gt;
&lt;h2&gt;Confidence Intervals&lt;/h2&gt;
&lt;p&gt;Using the critical value that we have computed for our test statistic, we can construct a confidence interval. We compute the confidence interval for a 2 sided test:&lt;/p&gt;
&lt;p&gt;For the z test:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/33e329d5fe3fc8ae5f252752af3efa1012fffed3/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7a2d63692e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/33e329d5fe3fc8ae5f252752af3efa1012fffed3/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7a2d63692e706e67&#34; alt=&#34;z ci&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/z-ci.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For the t test:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/7085c710c52f1243e4f60c63c267208ad592ba05/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f742d63692e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/7085c710c52f1243e4f60c63c267208ad592ba05/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f742d63692e706e67&#34; alt=&#34;t ci&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/t-ci.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;What do confidence intervals mean?&lt;/p&gt;
&lt;p&gt;For a 95% confidence interval it means that if we take our population and sample it 100 times, 95 times out of those hundred times, we will produce a confidence interval that contains the true mean of the population (remember that we don&#39;t know the true mean of the population, we are estimating it using the sample). If a large portion of our confidence intervals do not contain what our hypothesis claims to be the mean, then we can reject the null hypothesis.&lt;/p&gt;
&lt;h2&gt;Performing a Hypothesis Test&lt;/h2&gt;
&lt;p&gt;Exploring a more concrete example will help tie these concepts together. A pharmaceutical company is trying out a medication for lowering blood sugar and managing diabetes. It is known that any level of Hemoglobin A1c below 5.7% is considered normal. The drug company has treated 100 study volunteers with this medication and would like to prove that after treatment their mean A1c is below 5.7%.&lt;/p&gt;
&lt;p&gt;Our hypothesis will be:&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;H&lt;sub&gt;0&lt;/sub&gt;: &amp;mu; &amp;ge; 5.7&lt;/p&gt;
&lt;p&gt;H&lt;sub&gt;1&lt;/sub&gt;: &amp;mu; &amp;lt; 5.7&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The pharmaceutical company reports a mean A1c of 5.1% with a sample standard deviation of 1.6.&lt;/p&gt;
&lt;p&gt;We would like to use our sample to make inference regarding the population. If we reject the null hypothesis, this could provide us with some evidence that the average levels of A1c are within the normal range.&lt;/p&gt;
&lt;p&gt;In order to perform a hypothesis test, first we check our assumptions. We do not know anything about the population standard deviation, so even though the sample size is large enough, we will use the t test. We can assume that the pharmaceutical company used proper practices when selecting their study subjects in such a way to avoid dependence and that they are also aware that the data is derived from a normally distributed population.&lt;/p&gt;
&lt;p&gt;Our test:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/dd337d27334a650e4c9e1dba93985bbf8376ee73/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6578616d706c652d746573742d7374617469737469632e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/dd337d27334a650e4c9e1dba93985bbf8376ee73/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6578616d706c652d746573742d7374617469737469632e706e67&#34; alt=&#34;t ci&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/example-test-statistic.png&#34; style=&#34;max-width: 100%;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Since we have 100 subjects, we use 99 degrees of freedom to compute the test statistic. The test statistic for 99 degrees of freedom translates to a p-value of less than 0.0005. This is smaller than 0.05 and therefore we reject the null hypothesis. This is great news for our patients!&lt;/p&gt;
&lt;h2&gt;Hypothesis Tests in Python&lt;/h2&gt;
&lt;p&gt;We can use the Scipy library to perform hypothesis tests. The Scipy library has a function for one sample hypothesis tests called &lt;code&gt;ttest_1samp&lt;/code&gt;. This test takes a dataset and a constant for comparison and returns the test statistic and the p value for a 2 sided test.&lt;/p&gt;
&lt;p&gt;Our test is a one-sided test so we will only look at the test statistic. In order to use the p-value we have to divide the p-value by 2.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; from scipy.stats import ttest_1samp
&amp;gt;&amp;gt;&amp;gt; patients = np.random.normal(5.1, 1.6, 100)
&amp;gt;&amp;gt;&amp;gt; ttest_1samp(patients, 5.7)
Ttest_1sampResult(statistic=-3.47086065584463, pvalue=0.0007702018310937669)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we generated random data with mean 5.1 and standard deviation 1.6 in order to simulate our patients. Our test statistic is close but not exactly the same since the mean of the sample is not exactly 5.1 like in the example but in fact:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.mean(patients)
5.1666229732990345
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This explains the small discrepancy in the test statistic. However, the result is the same - we reject the null hypothesis.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this lesson we learned how to perform a hypothesis test. To do this, we had to introduce a large number of terms: population, sample, null hypothesis, alternative hypothesis, test statistic, p-value, and confidence interval. We then showed how we can use our test to come up with a conclusion for a real-life scenario and learned how to do this with Python.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_5"
    aria-labelledby="tab_5"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@8c498edf120c4361b348973fe5d8f0b4&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Two Sample Hypothesis Tests with Scipy&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@8c498edf120c4361b348973fe5d8f0b4&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@280a25bab93e4f8cb20861bba5430885&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@280a25bab93e4f8cb20861bba5430885&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;
&lt;a id=&#34;user-content-lesson-goals&#34; class=&#34;anchor&#34; href=&#34;#lesson-goals&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Lesson Goals&lt;/h2&gt;
&lt;p&gt;In this lesson we will continue to explore hypothesis tests and learn about performing tests with two samples of data.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-introduction&#34; class=&#34;anchor&#34; href=&#34;#introduction&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;It is common for researchers to perform studies to compare two groups and check if they behave differently. One example is applying a treatment to one group while another is left untreated. The results are then compared to see if the two groups differ. Hypothesis tests shine the most in such problems where we need to use statistics to inform decision making. In this lesson we will learn about the different ways we can compare two samples to see if they differ significantly.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-matched-pairs&#34; class=&#34;anchor&#34; href=&#34;#matched-pairs&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Matched Pairs&lt;/h2&gt;
&lt;p&gt;When we approach a two sample hypothesis test, we must first determine what type data we have. The first type of 2 sample hypothesis test is performed on matched pairs. This means that the data in the two samples is dependent. For example, in a clinical drug trial, we may give a blood pressure medication to a group of people and look at their blood pressure before and after the treatment. We will then treat the before and after as two samples and compare them. However, since the two groups both contain the same people, we are able to match each entry in the before data with its corresponding entry of the after data.&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-a-bit-of-theory&#34; class=&#34;anchor&#34; href=&#34;#a-bit-of-theory&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;A Bit of Theory&lt;/h3&gt;
&lt;p&gt;Since the we can match the data between the samples, we take the difference between the two samples in each row and then revert to using a one sample hypothesis test. Our hypothesis test will check whether the mean of the differences is significantly different from zero (we could also test that the mean is greater than or less than zero using a one-sided hypothesis test). For a two sided hypothesis test, this is our hypothesis:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/c50a3df136367c69c883f702f62759566ddbff0f/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6d6174636865642d70616972732d746573742e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/c50a3df136367c69c883f702f62759566ddbff0f/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6d6174636865642d70616972732d746573742e706e67&#34; alt=&#34;matched pairs hypothesis test&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/matched-pairs-test.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-matched-pairs-in-python&#34; class=&#34;anchor&#34; href=&#34;#matched-pairs-in-python&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Matched Pairs in Python&lt;/h3&gt;
&lt;p&gt;In our example we will look at a blood pressure study with 100 participants. Our participants all had their blood pressure measured before the beginning of the study and a month into the study. We will compare the systolic blood pressure for the participants before and after.&lt;/p&gt;
&lt;p&gt;Let&#39;s start by looking at our dataset. The file &lt;code&gt;blood_pressure.csv&lt;/code&gt; can be downloaded &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/data/module-2/blood_pressure.csv&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; pandas &lt;span class=&#34;pl-k&#34;&gt;as&lt;/span&gt; pd

blood_pressure &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; pd.read_csv(&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;blood_pressure.csv&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;)
blood_pressure.head()
       before       after
&lt;span class=&#34;pl-c1&#34;&gt;0&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;136.713072&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;92.432965&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;134.735618&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;105.022643&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;127.529115&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;82.242766&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;3&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;144.527126&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;93.607172&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;4&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;124.214720&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;103.212223&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We will be using the &lt;code&gt;scipy&lt;/code&gt; function &lt;code&gt;ttest_rel&lt;/code&gt;. This function is used for hypothesis testing of dependent data.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; ttest_rel

ttest_rel(blood_pressure.after, blood_pressure.before)
Ttest_relResult(&lt;span class=&#34;pl-v&#34;&gt;statistic&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;27.29184176756024&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;7.303035069607835e-48&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our result is a very small p-value. This means that we will reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;Since a matched pairs test is equivalent to a one sample test of the difference, we can also perform a one sample test and get the exact same result.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; ttest_1samp
ttest_1samp(blood_pressure.after&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;blood_pressure.before, &lt;span class=&#34;pl-c1&#34;&gt;0&lt;/span&gt;)
Ttest_1sampResult(&lt;span class=&#34;pl-v&#34;&gt;statistic&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;27.29184176756024&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;7.303035069607835e-48&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can see that the p-value is identical since the tests are equivalent.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-independent-samples&#34; class=&#34;anchor&#34; href=&#34;#independent-samples&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Independent Samples&lt;/h2&gt;
&lt;p&gt;The second type of two sample hypothesis tests are independent samples. In this case, we have two groups where we cannot match the rows to one another. For example, we compare the effect of a certain medication on a sample of men and a sample of women. We then perform a hypothesis test to see whether there is a significant difference in the way the medication affects the groups. Another example is an A/B test on a website. We can implement a number of changes in the UI of an e-commerce website. We will release version A to a sample of customers and version B to another sample. We will then test if there is a difference in revenue between the different samples.&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-a-bit-of-theory-1&#34; class=&#34;anchor&#34; href=&#34;#a-bit-of-theory-1&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;A Bit of Theory&lt;/h3&gt;
&lt;p&gt;When looking at two independent samples, we need to check that a few assumptions hold. The first assumption is obviously independence. An example of what could cause a dependence between two groups is if we had a study on the impact of nutrition on health and we had a husband in one group and a wife in the other. While they are not the same person, they most likely live in the same household. Therefore, there are some things that they do that might be similar like sleep habits or commuting habits. As researchers, when this happens, we cannot be sure whether the intervention in our study was the main cause of the difference (or similarity) between the subjects.&lt;/p&gt;
&lt;p&gt;With a 2 sample test, our hypothesis test (for a 2 sided test) is a comparison of the two means.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/00a915e374656e7c45ba7b59d3cc310d591ee55b/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f74776f2d73616d706c652d6879706f7468657369732e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/00a915e374656e7c45ba7b59d3cc310d591ee55b/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f74776f2d73616d706c652d6879706f7468657369732e706e67&#34; alt=&#34;2 sample hypothesis test&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/two-sample-hypothesis.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We must also assume that the samples were drawn at random from a normally distributed population.&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-equal-variances&#34; class=&#34;anchor&#34; href=&#34;#equal-variances&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Equal Variances&lt;/h3&gt;
&lt;p&gt;If we make an additional assumption that the variances of the two populations are equal, we may use a pooled standard deviation in our hypothesis test.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;scipy&lt;/code&gt;, this means that we will be setting &lt;code&gt;equal_var=True&lt;/code&gt; in our function.&lt;/p&gt;
&lt;p&gt;The following is an example of a 2 sample hypothesis test with equal variance. We will load a sample dataset of transaction amounts from an e-commerce website. The dataset &lt;code&gt;ab_test.csv&lt;/code&gt; can be downloaded &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/data/module-2/ab_test.csv&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;ab_test &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; pd.read_csv(&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;ab_test.csv&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;)
ab_test.head()
       a      b
&lt;span class=&#34;pl-c1&#34;&gt;0&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;0.27&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;13.61&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;6.08&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;21.53&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;13.74&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;9.23&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;3&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;9.70&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;5.36&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;4&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;7.00&lt;/span&gt;  &lt;span class=&#34;pl-c1&#34;&gt;12.90&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The rows are not matched and the data is not stored in any order.&lt;/p&gt;
&lt;p&gt;We make the assumption that the variances of both populations are equal based on prior knowledge of the data. Now we will test that there is a significant difference between the website layouts with a 95% degree of confidence.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; ttest_ind

ttest_ind(ab_test.a, ab_test.b, &lt;span class=&#34;pl-v&#34;&gt;equal_var&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;True&lt;/span&gt;)
Ttest_indResult(&lt;span class=&#34;pl-v&#34;&gt;statistic&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;2.637533181209767&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;0.009713140852447347&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our p-value is very small. This means that there is a significant difference between the two sample means.&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-unequal-variances&#34; class=&#34;anchor&#34; href=&#34;#unequal-variances&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Unequal Variances&lt;/h3&gt;
&lt;p&gt;When we don&#39;t feel comfortable that we can make the equal variance assumption with great certainty, we can use a more robust test instead. Instead of using a test with pooled variance, we use a test called Welch&#39;s t-test. This test is considered robust since it does not need to make as many assumptions about the data.&lt;/p&gt;
&lt;p&gt;Let&#39;s use our A/B test data to perform a t-test that does not require the equal variance assumption:&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;ttest_ind(ab_test.a, ab_test.b, &lt;span class=&#34;pl-v&#34;&gt;equal_var&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;False&lt;/span&gt;)
Ttest_indResult(&lt;span class=&#34;pl-v&#34;&gt;statistic&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;2.637533181209767&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;0.009776243024828825&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this case the p-value slightly differs from the one we get with equal variances. However, since it is very small in this case as well, we will still reject the null hypothesis and conclude that there is a significant difference between the two sample means.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-summary&#34; class=&#34;anchor&#34; href=&#34;#summary&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this lesson, we have looked at how to compare 3 different kinds of two sample tests. We first looked at matched pairs where our data was not independent. We then looked at the two different options for independent data. Hypothesis tests are an important tool in many areas of business and research. Therefore, it is important to master them and know to distinguish between the different types of tests.&lt;/p&gt;

&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_6"
    aria-labelledby="tab_6"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@798f359ad2524624879cde57e5129377&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;More Hypothesis Testing with SciPy and Stats Models&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@798f359ad2524624879cde57e5129377&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@d4ff70cc29204e849bc2fdc860b250ee&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@d4ff70cc29204e849bc2fdc860b250ee&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;
&lt;a id=&#34;user-content-lesson-goals&#34; class=&#34;anchor&#34; href=&#34;#lesson-goals&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Lesson Goals&lt;/h2&gt;
&lt;p&gt;In this lesson we will learn more about tests that we can perform with SciPy. These tests allow us to make decisions based on data and compare information in two or more variables.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-introduction&#34; class=&#34;anchor&#34; href=&#34;#introduction&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The field of statistics helps us make decisions using data. In previous lessons, we have looked at the comparison of one sample to a constant or the comparison of two samples to each other. In this lesson, we will use statistical tools to examine a number of features at once. We will also learn about linear regression using SciPy.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-anova-and-the-f-test&#34; class=&#34;anchor&#34; href=&#34;#anova-and-the-f-test&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;ANOVA and the F-Test&lt;/h2&gt;
&lt;p&gt;ANOVA (or ANalysis Of VAriance) is a technique meant to compare the means of three or more independent samples. An example of when we might use ANOVA is when conducting a test on an e-commerce website and trying out multiple UI designs at once to see if there is a change in sales.&lt;/p&gt;
&lt;p&gt;The hypothesis test that we are examining is:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/8ef2e56f4ff102812fab4f02f7c14332b4054fe2/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f616e6f76612d6879706f7468657369732e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/8ef2e56f4ff102812fab4f02f7c14332b4054fe2/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f616e6f76612d6879706f7468657369732e706e67&#34; alt=&#34;Anova Hypothesis Test&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/anova-hypothesis.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Where Ој represents a mean and there are a total of k means that we are comparing.&lt;/p&gt;
&lt;p&gt;Typically, the ANOVA is a table consisting of values that help us compute a p-value for our hypothesis. The p-value will be found by performing the F-test. The F-test is a test for comparing variances.&lt;/p&gt;
&lt;p&gt;Let&#39;s look at an example. Let&#39;s say we have the following data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Group 1&lt;/th&gt;
&lt;th&gt;Group 2&lt;/th&gt;
&lt;th&gt;Group 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Number of Samples&lt;/td&gt;
&lt;td&gt;n&lt;sub&gt;1&lt;/sub&gt;
&lt;/td&gt;
&lt;td&gt;n&lt;sub&gt;2&lt;/sub&gt;
&lt;/td&gt;
&lt;td&gt;n&lt;sub&gt;3&lt;/sub&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample Mean&lt;/td&gt;
&lt;td&gt;XМ…&lt;sub&gt;1&lt;/sub&gt;
&lt;/td&gt;
&lt;td&gt;XМ…&lt;sub&gt;2&lt;/sub&gt;
&lt;/td&gt;
&lt;td&gt;XМ…&lt;sub&gt;3&lt;/sub&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample Standard Deviation&lt;/td&gt;
&lt;td&gt;s&lt;sub&gt;1&lt;/sub&gt;
&lt;/td&gt;
&lt;td&gt;s&lt;sub&gt;2&lt;/sub&gt;
&lt;/td&gt;
&lt;td&gt;s&lt;sub&gt;3&lt;/sub&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We would like to compare these three samples and see whether there is a significant difference in at least one of them.&lt;/p&gt;
&lt;p&gt;With the ANOVA, we compare the difference in variation between the groups and the difference in variation within the groups themselves. If the F statistic is sufficiently large, this means the p-value will be sufficiently small. This will lead us to reject the null hypothesis and conclude that there is significant variation between the groups and therefore at least one of the means is different.&lt;/p&gt;
&lt;p&gt;This is how we would construct an ANOVA:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/f4be2d25ab16745e51ad1d15349badb3329ec78d/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f616e6f76612e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/f4be2d25ab16745e51ad1d15349badb3329ec78d/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f616e6f76612e706e67&#34; alt=&#34;Anova Table&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/anova.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-anova-in-python&#34; class=&#34;anchor&#34; href=&#34;#anova-in-python&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;ANOVA in Python&lt;/h2&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-using-scipy&#34; class=&#34;anchor&#34; href=&#34;#using-scipy&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Using SciPy&lt;/h3&gt;
&lt;p&gt;There are a number of ways to perform the ANOVA F-test in Python. The first way is using SciPy. We can pass all groups to the &lt;code&gt;f_oneway&lt;/code&gt; function. This function returns the result of the hypothesis test.&lt;/p&gt;
&lt;p&gt;Below is an example of a dataset containing 8 observations of car loan interest rates from 6 different cities. We would like to show that there is a difference in the rates based on city. The dataset &lt;code&gt;rate_by_city.csv&lt;/code&gt; can be obtained &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/data/module-2/rate_by_city.csv&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; pandas &lt;span class=&#34;pl-k&#34;&gt;as&lt;/span&gt; pd
&lt;span class=&#34;pl-k&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; f_oneway

&lt;span class=&#34;pl-c&#34;&gt;&lt;span class=&#34;pl-c&#34;&gt;#&lt;/span&gt;let&#39;s load the dataset&lt;/span&gt;
rate &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; pd.read_csv(&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;rate_by_city.csv&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;)
rate.head(&lt;span class=&#34;pl-c1&#34;&gt;15&lt;/span&gt;)
 	Rate 	City
&lt;span class=&#34;pl-c1&#34;&gt;0&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;3&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;4&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;5&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;6&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;7&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;8&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;9&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;14.25&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;10&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;11&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;12&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;13&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;14&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.40&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The dataset contains two columns - rate and city. To test our hypothesis, we need to either pass in multiple filtered subsets to our function or to pivot the dataset to have one column per city. We&#39;ll choose the second option. We&#39;ll start off by using the &lt;code&gt;cumcount&lt;/code&gt; function to create a new index and then use the &lt;code&gt;pivot&lt;/code&gt; function to create 6 city columns. We will then rename the columns to allow us to access them more easily.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;rate[&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;city_count&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;] &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; rate.groupby(&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;City&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;).cumcount()
rate_pivot &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; rate.pivot(&lt;span class=&#34;pl-v&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;city_count&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;City&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;Rate&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;)
rate_pivot.columns &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; [&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;City_&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;str&lt;/span&gt;(x) &lt;span class=&#34;pl-k&#34;&gt;for&lt;/span&gt; x &lt;span class=&#34;pl-k&#34;&gt;in&lt;/span&gt; rate_pivot.columns.values]
rate_pivot.head()
 	City_1 	City_2 	City_3 	City_4 	City_5 	City_6
city_count 						
&lt;span class=&#34;pl-c1&#34;&gt;0&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;14.25&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;14.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;15.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;14.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.50&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;14.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;14.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;14.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.25&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.51&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;14.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.25&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;3&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.59&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.90&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.00&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;4&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.00&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.50&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.25&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;13.75&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;12.00&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now that we have successfully pivoted the data, we can perform the test. The &lt;code&gt;f_oneway&lt;/code&gt; function requires us to specify each column that is passed into the function (rather than passing the entire dataframe)&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;f_oneway(rate_pivot.City_1,rate_pivot.City_2,rate_pivot.City_3,rate_pivot.City_4,rate_pivot.City_5,rate_pivot.City_6)
F_onewayResult(&lt;span class=&#34;pl-v&#34;&gt;statistic&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;4.8293848737024&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;pvalue&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;0.001174551414504048&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The p-value is 0.001174. This value is very small, certainly smaller than 0.05. Therefore, we reject the null hypothesis and conclude that the rates differ by city.&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-using-statsmodels&#34; class=&#34;anchor&#34; href=&#34;#using-statsmodels&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Using &lt;code&gt;statsmodels&lt;/code&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;statsmodels&lt;/code&gt; is a Python library aimed specifically at performing statistical tests and hypothesis testing. The output from this library tends to be more detailed.&lt;/p&gt;
&lt;p&gt;The function for generating an ANOVA in &lt;code&gt;statsmodels&lt;/code&gt; is called &lt;code&gt;anova_lm&lt;/code&gt; and it generates an ANOVA table. As a first step, we define a model and then generate the ANOVA table. In this case, we prefer not to pivot our data since the library will do it for us.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; statsmodels.api &lt;span class=&#34;pl-k&#34;&gt;as&lt;/span&gt; sm
&lt;span class=&#34;pl-k&#34;&gt;from&lt;/span&gt; statsmodels.formula.api &lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; ols

model &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; ols(&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;Rate ~ C(City)&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;rate).fit()
anova_table &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; sm.stats.anova_lm(model, &lt;span class=&#34;pl-v&#34;&gt;typ&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;)
anova_table
 	sum_sq 	df 	F 	PR(&lt;span class=&#34;pl-k&#34;&gt;&amp;gt;&lt;/span&gt;F)
C(City) 	&lt;span class=&#34;pl-c1&#34;&gt;10.945667&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;5.0&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;4.829385&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;0.001175&lt;/span&gt;
Residual 	&lt;span class=&#34;pl-c1&#34;&gt;21.758133&lt;/span&gt; 	&lt;span class=&#34;pl-c1&#34;&gt;48.0&lt;/span&gt; 	NaN 	NaN&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the code below, we defined a model of rate and city. The pivoting is performed internally by using the &lt;code&gt;C&lt;/code&gt; function. Our result is the same p-value and our conclusion to reject remains the same.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-linear-regression&#34; class=&#34;anchor&#34; href=&#34;#linear-regression&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Linear Regression&lt;/h2&gt;
&lt;p&gt;As we have previously seen, linear regression is a technique for modelling the relationship between one or more predictor (or independent) variables and one or more response (or dependent) variables. Our goal using linear regression is to explain the relationship using a linear equation of the form:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/14ded4d1fa56aa7e60e3318b01bfd64ed538c042/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f72656772657373696f6e2d6571756174696f6e2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/14ded4d1fa56aa7e60e3318b01bfd64ed538c042/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f72656772657373696f6e2d6571756174696f6e2e706e67&#34; alt=&#34;regression equation&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/regression-equation.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Using linear regression means having a simple and interpretable model at the cost of losing granular information and potentially oversimplifying and increasing our error.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-linear-regression-in-python&#34; class=&#34;anchor&#34; href=&#34;#linear-regression-in-python&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Linear Regression in Python&lt;/h2&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-linear-regression-using-scipy&#34; class=&#34;anchor&#34; href=&#34;#linear-regression-using-scipy&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Linear Regression using SciPy&lt;/h3&gt;
&lt;p&gt;There are many ways to perform regression in Python and this lesson will discuss both SciPy and statsmodels. We perform linear regression in SciPy using the &lt;code&gt;linregress&lt;/code&gt; function. This function returns the slope, the intercept, the r-value (which we will square to find r squared), the p-value (this test checks whether the slope is significantly different from zero), and the standard error of the estimated gradient.&lt;/p&gt;
&lt;p&gt;In the example below, we will create a linear model that predicts MPG using acceleration in the &lt;code&gt;auto-mpg&lt;/code&gt; dataset. Note that &lt;code&gt;linregress&lt;/code&gt; only supports linear regression with one variable for x and one for y.&lt;/p&gt;
&lt;p&gt;The dataset &lt;code&gt;auto-mpg.csv&lt;/code&gt; can be obtained &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/data/auto-mpg.csv&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from scipy.stats import linregress

auto = pd.read_csv(&#39;auto-mpg.csv&#39;)
auto.head()
 	mpg 	cylinders 	displacement 	horse_power 	weight 	acceleration 	model_year 	car_name
0 	18.0 	8 	307.0 	130.0 	3504 	12.0 	70 	\t&#34;chevrolet chevelle malibu&#34;
1 	15.0 	8 	350.0 	165.0 	3693 	11.5 	70 	\t&#34;buick skylark 320&#34;
2 	18.0 	8 	318.0 	150.0 	3436 	11.0 	70 	\t&#34;plymouth satellite&#34;
3 	16.0 	8 	304.0 	150.0 	3433 	12.0 	70 	\t&#34;amc rebel sst&#34;
4 	17.0 	8 	302.0 	140.0 	3449 	10.5 	70 	\t&#34;ford torino&#34;

slope, intercept, r_value, p_value, std_err = linregress(auto.acceleration, auto.mpg)
slope, intercept, r_value, p_value, std_err
(1.1912045293502271,
 4.969793004253912,
 0.17664276963558906,
 1.8230915350787203e-18,
 0.12923643283101396)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that our regression equation is:&lt;/p&gt;
&lt;p&gt;mpg = 4.9698 + 1.1912 * acceleration&lt;/p&gt;
&lt;p&gt;The r squared is 0.1766 which is relatively small. This means that our model only captures 17% of the variation in the data.&lt;/p&gt;
&lt;p&gt;The p-value is very small, this means that the slope is significantly different from zero.&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-linear-regression-using-statsmodels&#34; class=&#34;anchor&#34; href=&#34;#linear-regression-using-statsmodels&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Linear Regression using &lt;code&gt;statsmodels&lt;/code&gt;
&lt;/h3&gt;
&lt;p&gt;Unlike SciPy, the output we get with &lt;code&gt;statsmodels&lt;/code&gt; is more detailed. Below, we will repeat the same example but using &lt;code&gt;statsmodels&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; statsmodels.api &lt;span class=&#34;pl-k&#34;&gt;as&lt;/span&gt; sm

X &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; sm.add_constant(auto.acceleration) &lt;span class=&#34;pl-c&#34;&gt;&lt;span class=&#34;pl-c&#34;&gt;#&lt;/span&gt; We must add the intercept using the add_constant function&lt;/span&gt;
Y &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; auto.mpg

model &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; sm.OLS(Y, X).fit()
predictions &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; model.predict(X) 

print_model &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; model.summary()
&lt;span class=&#34;pl-c1&#34;&gt;print&lt;/span&gt;(print_model)

                            &lt;span class=&#34;pl-c1&#34;&gt;OLS&lt;/span&gt; Regression Results                            
&lt;span class=&#34;pl-k&#34;&gt;==============================================================================&lt;/span&gt;
Dep. Variable:                    mpg   R&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;squared:                       &lt;span class=&#34;pl-c1&#34;&gt;0.177&lt;/span&gt;
Model:                            &lt;span class=&#34;pl-c1&#34;&gt;OLS&lt;/span&gt;   Adj. R&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;squared:                  &lt;span class=&#34;pl-c1&#34;&gt;0.175&lt;/span&gt;
Method:                 Least Squares   F&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;statistic:                     &lt;span class=&#34;pl-c1&#34;&gt;84.96&lt;/span&gt;
Date:                Thu, &lt;span class=&#34;pl-c1&#34;&gt;31&lt;/span&gt; Jan &lt;span class=&#34;pl-c1&#34;&gt;2019&lt;/span&gt;   Prob (F&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;statistic):           &lt;span class=&#34;pl-c1&#34;&gt;1.82e-18&lt;/span&gt;
Time:                        &lt;span class=&#34;pl-c1&#34;&gt;13&lt;/span&gt;:&lt;span class=&#34;pl-c1&#34;&gt;29&lt;/span&gt;:&lt;span class=&#34;pl-c1&#34;&gt;47&lt;/span&gt;   Log&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;Likelihood:                &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;1343.9&lt;/span&gt;
No. Observations:                 &lt;span class=&#34;pl-c1&#34;&gt;398&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;AIC&lt;/span&gt;:                             &lt;span class=&#34;pl-c1&#34;&gt;2692&lt;/span&gt;.
Df Residuals:                     &lt;span class=&#34;pl-c1&#34;&gt;396&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;BIC&lt;/span&gt;:                             &lt;span class=&#34;pl-c1&#34;&gt;2700&lt;/span&gt;.
Df Model:                           &lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;                                         
Covariance Type:            nonrobust                                         
&lt;span class=&#34;pl-k&#34;&gt;================================================================================&lt;/span&gt;
                   coef    std err          t      P&lt;span class=&#34;pl-k&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;|&lt;/span&gt;t&lt;span class=&#34;pl-k&#34;&gt;|&lt;/span&gt;      [&lt;span class=&#34;pl-c1&#34;&gt;0.025&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.975&lt;/span&gt;]
&lt;span class=&#34;pl-ii&#34;&gt;--------------------------------------------------------------------------------&lt;/span&gt;
const            &lt;span class=&#34;pl-c1&#34;&gt;4.9698&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;2.043&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;2.432&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.015&lt;/span&gt;       &lt;span class=&#34;pl-c1&#34;&gt;0.953&lt;/span&gt;       &lt;span class=&#34;pl-c1&#34;&gt;8.987&lt;/span&gt;
acceleration     &lt;span class=&#34;pl-c1&#34;&gt;1.1912&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.129&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;9.217&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.000&lt;/span&gt;       &lt;span class=&#34;pl-c1&#34;&gt;0.937&lt;/span&gt;       &lt;span class=&#34;pl-c1&#34;&gt;1.445&lt;/span&gt;
&lt;span class=&#34;pl-k&#34;&gt;==============================================================================&lt;/span&gt;
Omnibus:                       &lt;span class=&#34;pl-c1&#34;&gt;17.459&lt;/span&gt;   Durbin&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;Watson:                   &lt;span class=&#34;pl-c1&#34;&gt;0.677&lt;/span&gt;
Prob(Omnibus):                  &lt;span class=&#34;pl-c1&#34;&gt;0.000&lt;/span&gt;   Jarque&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;Bera (&lt;span class=&#34;pl-c1&#34;&gt;JB&lt;/span&gt;):               &lt;span class=&#34;pl-c1&#34;&gt;18.214&lt;/span&gt;
Skew:                           &lt;span class=&#34;pl-c1&#34;&gt;0.497&lt;/span&gt;   Prob(&lt;span class=&#34;pl-c1&#34;&gt;JB&lt;/span&gt;):                     &lt;span class=&#34;pl-c1&#34;&gt;0.000111&lt;/span&gt;
Kurtosis:                       &lt;span class=&#34;pl-c1&#34;&gt;2.670&lt;/span&gt;   Cond. No.                         91.1
&lt;span class=&#34;pl-k&#34;&gt;==============================================================================&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here, we are not limited to only one predictor variable. Let&#39;s try this regression with more than one predictor.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;X &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; sm.add_constant(auto[[&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cylinders&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;weight&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;acceleration&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;]]) &lt;span class=&#34;pl-c&#34;&gt;&lt;span class=&#34;pl-c&#34;&gt;#&lt;/span&gt; adding a constant&lt;/span&gt;
Y &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; auto.mpg

model &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; sm.OLS(Y, X).fit()
predictions &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; model.predict(X) 

print_model &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; model.summary()
&lt;span class=&#34;pl-c1&#34;&gt;print&lt;/span&gt;(print_model)
                            &lt;span class=&#34;pl-c1&#34;&gt;OLS&lt;/span&gt; Regression Results                            
&lt;span class=&#34;pl-k&#34;&gt;==============================================================================&lt;/span&gt;
Dep. Variable:                    mpg   R&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;squared:                       &lt;span class=&#34;pl-c1&#34;&gt;0.700&lt;/span&gt;
Model:                            &lt;span class=&#34;pl-c1&#34;&gt;OLS&lt;/span&gt;   Adj. R&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;squared:                  &lt;span class=&#34;pl-c1&#34;&gt;0.698&lt;/span&gt;
Method:                 Least Squares   F&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;statistic:                     &lt;span class=&#34;pl-c1&#34;&gt;306.7&lt;/span&gt;
Date:                Thu, &lt;span class=&#34;pl-c1&#34;&gt;31&lt;/span&gt; Jan &lt;span class=&#34;pl-c1&#34;&gt;2019&lt;/span&gt;   Prob (F&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;statistic):          &lt;span class=&#34;pl-c1&#34;&gt;1.14e-102&lt;/span&gt;
Time:                        &lt;span class=&#34;pl-c1&#34;&gt;13&lt;/span&gt;:&lt;span class=&#34;pl-c1&#34;&gt;33&lt;/span&gt;:&lt;span class=&#34;pl-c1&#34;&gt;47&lt;/span&gt;   Log&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;Likelihood:                &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;1142.9&lt;/span&gt;
No. Observations:                 &lt;span class=&#34;pl-c1&#34;&gt;398&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;AIC&lt;/span&gt;:                             &lt;span class=&#34;pl-c1&#34;&gt;2294&lt;/span&gt;.
Df Residuals:                     &lt;span class=&#34;pl-c1&#34;&gt;394&lt;/span&gt;   &lt;span class=&#34;pl-c1&#34;&gt;BIC&lt;/span&gt;:                             &lt;span class=&#34;pl-c1&#34;&gt;2310&lt;/span&gt;.
Df Model:                           &lt;span class=&#34;pl-c1&#34;&gt;3&lt;/span&gt;                                         
Covariance Type:            nonrobust                                         
&lt;span class=&#34;pl-k&#34;&gt;================================================================================&lt;/span&gt;
                   coef    std err          t      P&lt;span class=&#34;pl-k&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;|&lt;/span&gt;t&lt;span class=&#34;pl-k&#34;&gt;|&lt;/span&gt;      [&lt;span class=&#34;pl-c1&#34;&gt;0.025&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.975&lt;/span&gt;]
&lt;span class=&#34;pl-ii&#34;&gt;--------------------------------------------------------------------------------&lt;/span&gt;
const           &lt;span class=&#34;pl-c1&#34;&gt;42.3811&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;1.960&lt;/span&gt;     &lt;span class=&#34;pl-c1&#34;&gt;21.627&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.000&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;38.528&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;46.234&lt;/span&gt;
cylinders       &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;0.4827&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.302&lt;/span&gt;     &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;1.599&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.111&lt;/span&gt;      &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;1.076&lt;/span&gt;       &lt;span class=&#34;pl-c1&#34;&gt;0.111&lt;/span&gt;
weight          &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;0.0065&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.001&lt;/span&gt;    &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;11.342&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.000&lt;/span&gt;      &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;0.008&lt;/span&gt;      &lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;0.005&lt;/span&gt;
acceleration     &lt;span class=&#34;pl-c1&#34;&gt;0.2034&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.091&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;2.236&lt;/span&gt;      &lt;span class=&#34;pl-c1&#34;&gt;0.026&lt;/span&gt;       &lt;span class=&#34;pl-c1&#34;&gt;0.025&lt;/span&gt;       &lt;span class=&#34;pl-c1&#34;&gt;0.382&lt;/span&gt;
&lt;span class=&#34;pl-k&#34;&gt;==============================================================================&lt;/span&gt;
Omnibus:                       &lt;span class=&#34;pl-c1&#34;&gt;34.469&lt;/span&gt;   Durbin&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;Watson:                   &lt;span class=&#34;pl-c1&#34;&gt;0.816&lt;/span&gt;
Prob(Omnibus):                  &lt;span class=&#34;pl-c1&#34;&gt;0.000&lt;/span&gt;   Jarque&lt;span class=&#34;pl-k&#34;&gt;-&lt;/span&gt;Bera (&lt;span class=&#34;pl-c1&#34;&gt;JB&lt;/span&gt;):               &lt;span class=&#34;pl-c1&#34;&gt;45.516&lt;/span&gt;
Skew:                           &lt;span class=&#34;pl-c1&#34;&gt;0.654&lt;/span&gt;   Prob(&lt;span class=&#34;pl-c1&#34;&gt;JB&lt;/span&gt;):                     &lt;span class=&#34;pl-c1&#34;&gt;1.31e-10&lt;/span&gt;
Kurtosis:                       &lt;span class=&#34;pl-c1&#34;&gt;4.016&lt;/span&gt;   Cond. No.                     2.82e&lt;span class=&#34;pl-k&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;0&lt;span class=&#34;pl-ii&#34;&gt;4&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;pl-k&#34;&gt;==============================================================================&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-conclusion&#34; class=&#34;anchor&#34; href=&#34;#conclusion&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this lesson we learned how to create and evaluate an ANOVA table in both SciPy and &lt;code&gt;statsmodels&lt;/code&gt;. We learned the proper use of an F test and what hypothesis is tested using this test. We also looked at linear regression in both SciPy and &lt;code&gt;statsmodels&lt;/code&gt;. We were able to compare the more succinct output from SciPy with the detailed tables in &lt;code&gt;statsmodels&lt;/code&gt;. Both outputs serve a different purpose and have value in different scenarios. Hopefully, this lesson will empower you to use your statistics chops to make business decisions.&lt;/p&gt;

&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_7"
    aria-labelledby="tab_7"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@135c4ad869664502a9b5ceadbb524938&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Regression&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@135c4ad869664502a9b5ceadbb524938&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@730070415e33421ba706ba686bdb3ea7&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@730070415e33421ba706ba686bdb3ea7&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;Lesson Goals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learn linear regression to make predictions&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;font-size: 1em;&#34;&gt;Learn logistic regression to work on classification problems&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Learn techniques to manipulate numerical and categorical data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Regression analysis is one of the most common techniques that is used by data analysts to make predictions. We&amp;nbsp;will see how regression analysis works by establishing a relationship between the target variable (Y) and&amp;nbsp;one or more predictor variables (X&#39;s) to help us understand the functional dependence and thus make predictions or classification.&amp;nbsp;Based on the kind of relation between the two, we can make a broad classification as:&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt; - Here the relationship between X and Y is linear&amp;nbsp;&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;strong&gt;Non-Linear Regression&lt;/strong&gt; - Here the relationship between X and Y is non-linear&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Predictive Regression Analysis&lt;/h2&gt;
&lt;p&gt;One of the most common goals in statistics and data analysis is to answer the question, is there a relation between one or more factors associated with a target variable Y. And if there is, can we use that relationship to predict Y? Regression analysis is one of the most used techniques to establish this relation and make predictions.&lt;/p&gt;
&lt;h3&gt;Simple Linear Regression (Uni-variate Analysis):&lt;/h3&gt;
&lt;p&gt;With simple linear regression we predict the changes in Y with changes in X using a linear relationship. The equation resembles the equation of a straight line.&amp;nbsp;&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;img height=&#34;29&#34; width=&#34;115&#34; src=&#34;/assets/courseware/v1/6c6476a3983c91377f49289128efdea7/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Simple_Linear_regression_equation.png&#34; alt=&#34;Linear regression equation&#34; style=&#34;caret-color: #000000; color: #000000; font-family: -webkit-standard;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some of the terms associated with linear regression models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X - Independent variable / predictor / feature / attributes&lt;/li&gt;
&lt;li&gt;Y - Dependent variable / &amp;nbsp;response / target / outcome&lt;/li&gt;
&lt;li&gt;Regression Coefficients - b0 (intercept) ;&amp;nbsp;b1 (slope)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we will use the &lt;a href=&#34;https://github.com/haggarw3/Datasets/blob/master/Fitbit2.csv&#34; target=&#34;_blank&#34;&gt;Fitbit2.csv&lt;/a&gt;&amp;nbsp;file and look at two specific columns, &amp;lsquo;MinutesOfBeingAwake&amp;rsquo; and &amp;lsquo;NumberOfAwakings&amp;rsquo;. This is what a scatter plot between the two variables look like.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;img height=&#34;237&#34; width=&#34;267&#34; src=&#34;/assets/courseware/v1/76de806d86f2a6b4c67f55456ef6a618/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Linear_Regression_Figure.png&#34; alt=&#34;Linear Regression Line with the scatter plot of X and Y&#34; /&gt;&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;#import required libraries
#load the data set
sns.regplot(x=&#34;MinutesOfBeingAwake&#34;, y=&#34;NumberOfAwakings&#34;, data=data)&lt;/pre&gt;
&lt;p&gt;It looks like there&#39;s&amp;nbsp;positive linear relation between the two variables. If we establish a linear mathematical equation/relation between the two variables, it will be:&lt;/p&gt;
&lt;pre&gt;[MinutesOfBeingAwake] = b0 + b1*[NumberOfAwakings]&lt;/pre&gt;
&lt;p&gt;Python code example:&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;from sklearn import linear_model
lm = linear_model.LinearRegression()
X = pd.DataFrame(data= data, columns=[&#39;MinutesOfBeingAwake&#39;])
Y = pd.DataFrame(data= data, columns=[&#39;NumberOfAwakings&#39;])
model = lm.fit(X,Y)
lm.intercept_
lm.coef_
predictions  = lm.predict(X)&lt;/pre&gt;
&lt;h4&gt;Fitted Values and Residuals&lt;/h4&gt;
&lt;p&gt;The fitted values (predicted values), denoted by y-hat, are the values that the model fits on the data to get the straight line. These values lie actually on the line.&lt;/p&gt;
&lt;p&gt;Residual is the difference between the original value and the predicted value (errors of the prediction)&amp;nbsp;&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;img height=&#34;35&#34; width=&#34;106&#34; src=&#34;/assets/courseware/v1/2f58b24a9b99b3ae51cbd4024534ac30/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Linear_Regression_residuals.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;How to find the best fitting model?&lt;/h4&gt;
&lt;p&gt;The question is how do we draw that straight line that will pass through the data to represent the relation between the two most accurately. For that we need to find the slope and the intercept. Least Squares Estimation or commonly known as OLS (ordinary least squares) finds the best line that fits the data such that the sum of the squared errors/residuals is minimized. &amp;nbsp;It is important to note that even tho this method is computationally inexpensive, but it is sensitive to outliers. &amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Model Accuracy&amp;nbsp;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;RMSE (Root Mean Squared Error)&lt;/strong&gt; &amp;nbsp;measures the overall accuracy of the model. It is the square root of the average squared error calculated as:&amp;nbsp;&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;img height=&#34;54&#34; width=&#34;217&#34; src=&#34;/assets/courseware/v1/0116bdc4e37c7c40cafbe2ca26e7d695/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/RMSE.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;R square&lt;/strong&gt;: This is another important statistic used to measure the accuracy of the model. It ranges from 0 to 1 and it measures the proportion of variation in the data that the model is able to capture.&amp;nbsp;Usually the larger the value of R square, better is the model. &amp;nbsp;But this is not always true. We will talk about this in more detail later in this lesson.&amp;nbsp;&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;from sklearn.metrics import mean_squared_error, r2_score
print(mean_squared_error(Y, predictions))
r2_score(Y, predictions)&lt;/pre&gt;
&lt;pre&gt;&lt;/pre&gt;
&lt;p&gt;Some of the assumptions of the Linear regression model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;font-size: 1em;&#34;&gt;Relationship between the outcomes and the predictors is linear.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Errors are uncorrelated&lt;/li&gt;
&lt;li&gt;Errors are normally distributed with mean 0 and constant variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To check these assumptions we use residual analysis plots such as&amp;nbsp;QQ plot / Quantile Plots, Normal probability plots of the residuals, plot of, and plot of residuals vs fitted values. We will not go over them in this class but it was worth mentioning.&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Using Python Scikit Learn to find&amp;nbsp;summary()&lt;/h4&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;import statsmodels.api as sm
from statsmodels.formula.api import ols
model = ols(&#34;NumberOfAwakings~MinutesOfBeingAwake&#34;,data=data).fit()
print(model.summary())&lt;/pre&gt;
&lt;pre&gt;&lt;/pre&gt;
&lt;h3&gt;Multiple Linear Regression (Multivariate Analysis)&lt;/h3&gt;
&lt;p&gt;The concept is similar to simple linear regression except that here we have more that one independent variables X. The relation between the independent variables and the dependent variable is linear. &amp;nbsp;&lt;/p&gt;
&lt;p style=&#34;text-align: left; padding-left: 30px;&#34;&gt;&lt;img height=&#34;28&#34; width=&#34;259&#34; src=&#34;/assets/courseware/v1/28b34539f5de8ef36baaf2eef30c9956/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Multiple_Linear_Regrssion.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Y is still the same.&amp;nbsp;We will use additional columns from the dataset to include&amp;nbsp;as the predictor variables (hence multiple linear regression).&amp;nbsp;Please note that for nor we are only using the columns that&amp;nbsp;have numerical values.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Python code example:&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;Y = pd.DataFrame(data = data, columns = [&#39;NumberOfAwakings&#39;])
X = data[[&#39;MinutesOfBeingAwake&#39;, &#39;MinutesOfSleep&#39;, &#39;Activity Calories&#39;]]
lm = linear_model.LinearRegression()
model = lm.fit(X,Y)
lm.intercept_
lm.coef_
predictions  = lm.predict(X)&lt;br /&gt;&lt;br /&gt;from sklearn.metrics import mean_squared_error, r2_score
print(mean_squared_error(Y, predictions))
r2_score(Y, predictions)&lt;/pre&gt;
&lt;p&gt;Note: &lt;strong&gt;Hypothesis Testing in Multiple Regression&lt;/strong&gt; -&amp;nbsp;T statistics for coefficient is used to check if the coefficient is statistically significant. This acts as an important tool to check if the predictor variable associated with the coefficient is to be included in the model or not. &amp;nbsp;We will talk in more detail about in later in feature selection lesson.&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Data Preprocessing&lt;/h2&gt;
&lt;p&gt;Before we can actually use the predictor variables to build prediction or classification models, we need to make sure that the variables are processed so that there is no bias between the variables of importance. Also it is very important to understand and use the information from categorical variables&amp;nbsp;to achieve better accuracy.&amp;nbsp;Therefore it is very crucial to understand how to manipulate both numerical and categorical variables.&lt;/p&gt;
&lt;h3&gt;Scaling / Normalization&lt;/h3&gt;
&lt;p&gt;In the multilinear regression model, the predictor variables ie MinutesOfBeingAwake, &amp;nbsp;MinutesOfSleep, and Activity Calories&amp;nbsp;have different scales. A variable with a larger scale will try to influence the model more than the others even if&amp;nbsp;we know that all the variables are of significance.&amp;nbsp;Therefore scaling or normalization of the variables is performed.&amp;nbsp;Another advantage of scaling / standardizing is that it helps the models (regression/neural networks etc) to converge faster.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scale:&lt;/strong&gt; It means to change the range of values based on the minimum and the maximum values in the column&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;img height=&#34;56&#34; width=&#34;180&#34; src=&#34;/assets/courseware/v1/89a58fe3e414dc9117d6d2a872b08e19/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Scaling.png&#34; alt=&#34;Scaling numerical variables&#34; /&gt;&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;from sklearn.preprocessing import MinMaxScaler
transformer = MinMaxScaler().fit(data[[&#39;MinutesOfBeingAwake&#39;,&#39;MinutesOfSleep&#39;]])
transformer.transform(data[[&#39;MinutesOfBeingAwake&#39;,&#39;MinutesOfSleep&#39;]])&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Standardize / Normalize&lt;/strong&gt;: It means massaging the values so that the series now has a mean of 0 and sd = 1&amp;nbsp;&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;img height=&#34;48&#34; width=&#34;120&#34; src=&#34;/assets/courseware/v1/fad37b9a412d2d2e1299dc02c338c777/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Normalize.png&#34; alt=&#34;Normalizing Numerical variable &#34; /&gt;&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;from sklearn.preprocessing import Normalizer
transformer = Normalizer().fit(data[[&#39;MinutesOfBeingAwake&#39;,&#39;MinutesOfSleep&#39;]])
transformer.transform(data[[&#39;MinutesOfBeingAwake&#39;,&#39;MinutesOfSleep&#39;]])&amp;nbsp;&lt;/pre&gt;
&lt;h3&gt;Handling Categorical Variables&lt;/h3&gt;
&lt;p&gt;Usually when we are dealing with real world datasets, the data is not always only numerical. We also have categorical variables that affect the outcome / target variable. Categorical data has values that can be clearly categorized / segmented in a groups. &amp;nbsp;Different categories in a categorical variable are referred to as levels by data science community.&amp;nbsp;&lt;span style=&#34;font-size: 1em;&#34;&gt;Different kinds of categorical variables include:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;Apple-tab-span&#34; style=&#34;white-space: pre;&#34;&gt; &lt;/span&gt;&lt;strong&gt;Nominal&lt;/strong&gt; - These categorical variables that do not have an intrinsic order associated with them for eg type of OS in computers, gender&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ordinal&lt;/strong&gt; - These categorical variables that have an intrinsic order associated with them for eg no. of cylinders in a car;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our multi-linear model now we will try to include more predictor variables and check how will it affect the accuracy measures of the model. We will add &amp;ldquo;Months&amp;rdquo; and &amp;ldquo;Days&amp;rdquo; in addition to the numerical data columns (MinutesOfBeingAwake&#39;, &#39;MinutesOfSleep&#39;, &#39;Activity Calories&amp;rsquo;).&amp;nbsp;To include categorical variables in the regression model, we need to convert them into numeric values. Some of the techniques to do this are:&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;span class=&#34;Apple-tab-span&#34; style=&#34;white-space: pre;&#34;&gt; &lt;/span&gt;1.&lt;span class=&#34;Apple-tab-span&#34; style=&#34;white-space: pre;&#34;&gt; &lt;/span&gt;&lt;strong&gt;Label Encoding&lt;/strong&gt;: Categories are numbered from 0 to n-1 where &amp;rsquo;n&amp;rsquo; is the number of different categories/labels. Even though it is a much simpler concept of encoding, but it has the disadvantage that unwanted category weights are created. For eg in Months column, December will carry more weight as the numerical value associated with it will be 11 instead of January which will have the value of 1&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;pd.Categorical(data[&#39;Months&#39;]).codes&lt;span class=&#34;Apple-tab-span&#34; style=&#34;white-space: pre;&#34;&gt;&lt;/span&gt;&lt;/pre&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;span class=&#34;Apple-tab-span&#34; style=&#34;white-space: pre;&#34;&gt; &lt;/span&gt;2.&lt;span class=&#34;Apple-tab-span&#34; style=&#34;white-space: pre;&#34;&gt; &lt;/span&gt;&lt;strong&gt;One hot encoding&lt;/strong&gt;: &amp;nbsp;This method involves making a separate column for each category and assign a 1 or 0 (True/False) value to the column. The new columns are called dummies. Its advantage is that it does not apply a weighted value improperly but does have the downside of adding more columns to the data set.&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;pd.get_dummies(data3, columns=[&#34;Days&#34;]).head()&lt;br /&gt;&lt;br /&gt;&lt;/pre&gt;
&lt;h3&gt;Logistic Regression Analysis&lt;/h3&gt;
&lt;p&gt;There is another kind of problem where the output is categorical. These problems are also known as classification/segmentation problem. For eg consider the churning example for credit card companies. Based on the historical data, they want to find out if the customer will continue using the card after a year or not.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This problem can be answered by labeling the customers as Yes (will churn) or a No (will not churn). This is. a classic binary classification problem ie when the output choices are two.&amp;nbsp;Similarly when the output choices are more than 2, it is called Multi-class classification.&lt;/p&gt;
&lt;p&gt;Logistic Regression is an important technique that helps us to develop classification models. Today we will take up binary classification. &amp;nbsp;&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;&lt;img height=&#34;166&#34; width=&#34;288&#34; src=&#34;/assets/courseware/v1/9d2908f11e4111f2d43ff78133dc6c20/asset-v1:IRONHACK+DAFT+201906_MIA+type@asset+block/Logistic_Regression_Sigmoid_function.png&#34; alt=&#34;Logit&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using Python to make Logistic Regression Model&lt;/p&gt;
&lt;p&gt;We will use &lt;a href=&#34;https://github.com/haggarw3/Datasets/blob/master/Customer-Churn.csv&#34; target=&#34;_blank&#34;&gt;churn data&lt;/a&gt;&amp;nbsp;for this example.&amp;nbsp;&lt;/p&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;from sklearn.linear_model import LogisticRegression
churnData = pd.read_csv(&#39;~path/Customer-Churn.csv&#39;)
numericData = churnData[[&#39;tenure&#39;,&#39;SeniorCitizen&#39;,&#39;MonthlyCharges&#39;]]
Y = pd.DataFrame(data=churnData, columns=[&#39;Churn&#39;])
transformer = StandardScaler().fit(churnData[[&#39;tenure&#39;,&#39;SeniorCitizen&#39;,&#39;MonthlyCharges&#39;]])
scaled_x = transformer.transform(churnData[[&#39;tenure&#39;,&#39;SeniorCitizen&#39;,&#39;MonthlyCharges&#39;]])
classification = LogisticRegression(random_state=0, solver=&#39;lbfgs&#39;,
                        multi_class=&#39;ovr&#39;).fit(scaled_x, churnData[&#39;Churn&#39;])
classification.score(scaled_x, churnData[&#39;Churn&#39;])&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this lesson we saw linear regression and logistic regression techniques along with their application to solve prediction problems and classification problems respectively. We also learnt how to work with numerical data and categorical data so that it can be used in the model more appropriately.&lt;/p&gt;
&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_8"
    aria-labelledby="tab_8"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@a9cecb6aedca4e24aab92242dfff6300&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Introduction to Bayesian Statistics&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@a9cecb6aedca4e24aab92242dfff6300&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@6da07a84e786411eae464d2be89c16be&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@6da07a84e786411eae464d2be89c16be&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;
&lt;a id=&#34;user-content-lesson-goals&#34; class=&#34;anchor&#34; href=&#34;#lesson-goals&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Lesson Goals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Understand how conditional probabilities form the basis for Bayesian statistics.&lt;/li&gt;
&lt;li&gt;Introduce Bayes&#39; Theorem and its components.&lt;/li&gt;
&lt;li&gt;Use Bayes&#39; Theorem to arrive at an estimate.&lt;/li&gt;
&lt;li&gt;Perform a Bayesian analysis using a real-world scenario.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-introduction&#34; class=&#34;anchor&#34; href=&#34;#introduction&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the Introduction to Probability lesson, we introduced conditional probability, which is a prerequisite for Bayesian Statistics.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/61b2e4ad0950e7fbbde363ca0087324752bd33cc/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f636f6e646974696f6e616c2d70726f626162696c69746965732e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/61b2e4ad0950e7fbbde363ca0087324752bd33cc/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f636f6e646974696f6e616c2d70726f626162696c69746965732e706e67&#34; alt=&#34;Conditional Probabilities&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/conditional-probabilities.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A conditional probability is a probability based on some background information. It&#39;s notation is typically expressed as P(A|B), which means the probability of event A occurring given that some condition B is true. We saw that this could be computed using the following equation.&lt;/p&gt;
&lt;pre lang=&#34;text&#34;&gt;&lt;code&gt;P(A|B) = P(A&amp;cap;B) / P(B)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, the probability of A given B equals the probability of the intersection of A and B divided by the probability of B.&lt;/p&gt;
&lt;p&gt;We also briefly touched on the independence and dependence of events and said that when events are independent, at least one of the following must be true.&lt;/p&gt;
&lt;pre lang=&#34;text&#34;&gt;&lt;code&gt;P(A|B) = P(A)
P(B|A) = P(B)
P(A&amp;cap;B) = P(A) P(B)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, if the events are truly independent, then the probability of one given another should just be the probability of that event occurring. Similarly, the probability of the intersection of the two events should just be the product of the probabilities of the two events, since neither one impacts the other.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-bayes-theorem&#34; class=&#34;anchor&#34; href=&#34;#bayes-theorem&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Bayes&#39; Theorem&lt;/h2&gt;
&lt;p&gt;For dependent events, we need to modify the last set of equations to account for the fact that one event impacts the other.&lt;/p&gt;
&lt;pre lang=&#34;text&#34;&gt;&lt;code&gt;P(A&amp;cap;B) = P(A) P(B|A)
P(B&amp;cap;A) = P(B) P(A|B)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the intersection of A and B equals the probability of A times the probability of B given A and, since there is nothing special about A versus B, it also equals the probability of B times the probability of A given B.&lt;/p&gt;
&lt;p&gt;If this is the case, then:&lt;/p&gt;
&lt;pre lang=&#34;text&#34;&gt;&lt;code&gt;P(B) P(A|B) = P(A) P(B|A)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if we divide both sides by P(B), we end up with the following.&lt;/p&gt;
&lt;pre lang=&#34;text&#34;&gt;&lt;code&gt;P(A|B) = P(A) P(B|A) / P(B)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This equation is known as Bayes&#39; Theorem, and it is a surprisingly powerful equation that forms the fundamental idea behind all Bayesian statistics. It allows us to establish what we believe about the probability of a hypothesis prior to seeing any data, and it also provides us with a means by which to update those beliefs once we have seen some data.&lt;/p&gt;
&lt;p&gt;This equation has a few different components, so let&#39;s break those down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;code&gt;P(A)&lt;/code&gt; is known as the &lt;em&gt;prior probability&lt;/em&gt;, and it is the the probability of the hypothesis before we see the data.&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;P(B|A)&lt;/code&gt; is known as the &lt;em&gt;likelihood&lt;/em&gt;, and it represents the probability of the data under the hypothesis.&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;P(B)&lt;/code&gt; is known as the &lt;em&gt;marginal probability&lt;/em&gt;, and it is a normalizing constant that represents the probability of the data under any hypothesis, or P(A&amp;cup;B). In other words, it can be computed as &lt;code&gt;P(A) P(B|A) + P(-A) P(B|-A)&lt;/code&gt; where &lt;code&gt;-A&lt;/code&gt; is Not A (the alternative scenario of the hypothesis).&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;P(A|B)&lt;/code&gt; is known as the &lt;em&gt;posterior probability&lt;/em&gt;, and it is the probability that we want to compute after having seen the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; It is important to emphasize here that the &lt;em&gt;hypothesis&lt;/em&gt; is represented by &lt;code&gt;A&lt;/code&gt; and the &lt;em&gt;observed data&lt;/em&gt; is represented by &lt;code&gt;B&lt;/code&gt;. A common mistake and point of confusion is to think that &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; represent two different hypotheses, which is not the case. The two opposing hypotheses are represented by &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;-A&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can define a function in Python that accepts a list of priors and corresponding likelihoods. Our function will compute the marginal (&lt;code&gt;marg&lt;/code&gt;) and then use Bayes&#39; Theorem to calculate the posterior probabilities for the each of the hypotheses (&lt;code&gt;post&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; numpy &lt;span class=&#34;pl-k&#34;&gt;as&lt;/span&gt; np

&lt;span class=&#34;pl-k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;pl-en&#34;&gt;bayes_rule&lt;/span&gt;(&lt;span class=&#34;pl-smi&#34;&gt;priors&lt;/span&gt;, &lt;span class=&#34;pl-smi&#34;&gt;likelihoods&lt;/span&gt;):
    marg &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;pl-c1&#34;&gt;sum&lt;/span&gt;(np.multiply(priors, likelihoods))
    post &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; np.divide(np.multiply(priors, likelihoods), marg)
    &lt;span class=&#34;pl-k&#34;&gt;return&lt;/span&gt; post&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-application-of-bayes-theorem&#34; class=&#34;anchor&#34; href=&#34;#application-of-bayes-theorem&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Application of Bayes&#39; Theorem&lt;/h2&gt;
&lt;p&gt;Now that we have our function, let&#39;s propose a scenario and use our function to help us solve for the posterior probabilities.&lt;/p&gt;
&lt;p&gt;Suppose we have 3 jars with 100 marbles in each.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jar 1 has 40 blue marbles, 30 red marbles, and 30 green marbles.&lt;/li&gt;
&lt;li&gt;Jar 2 has 60 blue marbles, 20 red marbles, and 20 green marbles.&lt;/li&gt;
&lt;li&gt;Jar 3 has 10 blue marbles, 30 red marbles, and 70 green marbles.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You pick a marble from one of the jars at random, and the marble is green. What are the probabilities that the marble came from each of the three jars?&lt;/p&gt;
&lt;p&gt;To answer this question, we would need to know what our prior probabilities were and our likelihoods. For the priors, we can assume that the probability we drew the marble from any one jar is equally probably, or 1/3. The likelihoods in this case would just be the percentages of green marbles in each jar. We can plug these values into our &lt;code&gt;bayes_rule&lt;/code&gt; function, and it will return the following outputs.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;priors &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; [&lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;3&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;3&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;3&lt;/span&gt;]
likelihoods &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; [&lt;span class=&#34;pl-c1&#34;&gt;0.3&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;0.2&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;0.7&lt;/span&gt;]

bayes_rule(priors, likelihoods)

array([&lt;span class=&#34;pl-c1&#34;&gt;0.25&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;0.16666667&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;0.58333333&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From these results, we can see that there is a 25% chance that the green marble we drew came from Jar 1, a 16.67% chance that it came from Jar 2, and a 58.3% chance that it came from Jar 3.&lt;/p&gt;
&lt;p&gt;Suppose you had picked a blue marble instead of a green one. We would just need to update our likelihoods to account for the proportion of each jar whose marbles were blue.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;likelihoods &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; [&lt;span class=&#34;pl-c1&#34;&gt;0.4&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;0.6&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;0.1&lt;/span&gt;]

bayes_rule(priors, likelihoods)

array([&lt;span class=&#34;pl-c1&#34;&gt;0.36363636&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;0.54545455&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;0.09090909&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There&#39;s a 36.36% chance that the green marble we chose came from Jar 1, a 54.54% chance it came from Jar 2, and only a 9% chance that it came from Jar 3.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-bayesian-data-analysis&#34; class=&#34;anchor&#34; href=&#34;#bayesian-data-analysis&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Bayesian Data Analysis&lt;/h2&gt;
&lt;p&gt;The previous example was a simple one with the objective of demonstrating how Bayes&#39; Theorem works. When doing Bayesian Data Analysis in the real world, the priors and the posteriors are typically probability distributions. We typically have some data, and the way to get from the prior to the posterior is typically a generative model (one that accepts parameters and generates data from them).&lt;/p&gt;
&lt;p&gt;To get a sense of how this works, let&#39;s look at an example. Suppose we have just launched an online store that sells fidget spinners, and we are trying to estimate what percentage of visitors to our online store will make a purchase. We can set this up as a Bayesian problem and arrive at a probability distribution as follows.&lt;/p&gt;
&lt;p&gt;Prior to seeing any traffic or purchase data, we have no idea what to expect so we will start with the assumption that all percentages are equally likely. This is called a uniform prior because we can use a uniform distribution to represent the fact that every rate has an equal chance of being the actual rate of purchase. To set this up in Python, we are going to use Numpy to generate a uniform distribution from 100,000 random draws of numbers between 0 and 1.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;import&lt;/span&gt; pandas &lt;span class=&#34;pl-k&#34;&gt;as&lt;/span&gt; pd

n_draws &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;pl-c1&#34;&gt;100000&lt;/span&gt;
prior &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; pd.Series(np.random.uniform(&lt;span class=&#34;pl-c1&#34;&gt;0&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;1&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;n_draws))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is our prior distribution, and it looks like this.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;prior.hist()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/22c945fae2210a5348cca824e23e3a2fb8699ca4/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7072696f722d646973747269627574696f6e2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/22c945fae2210a5348cca824e23e3a2fb8699ca4/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7072696f722d646973747269627574696f6e2e706e67&#34; alt=&#34;Prior Distribution&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/prior-distribution.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now it&#39;s time to observe some data and see how this affects our estimates. Suppose on the first day, 50 people visited our site and 10 of them made a purchase. We are going to record the number of purchases in a variable called &lt;code&gt;observed&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;observed &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;pl-c1&#34;&gt;10&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ware also going to create a generative model that will randomly draw from our prior uniform distribution, simulate 50 people coming to our website a large number of time, and see how many times we get a result that is in line with the result we have observed. To do this in Python, we are going to define a &lt;code&gt;generative_model&lt;/code&gt; function that accepts a random probability parameter from our prior and then performs 50 binomial draws using that probability.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;pl-en&#34;&gt;generative_model&lt;/span&gt;(&lt;span class=&#34;pl-smi&#34;&gt;param&lt;/span&gt;):
    result &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; np.random.binomial(&lt;span class=&#34;pl-c1&#34;&gt;50&lt;/span&gt;, param)
    &lt;span class=&#34;pl-k&#34;&gt;return&lt;/span&gt; result&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We are going to create a empty list that is going to contain our simulated results (&lt;code&gt;sim_data&lt;/code&gt;) and then populate it by appending the results when each value in our prior distribution is plugged into our generative model.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;sim_data &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;pl-c1&#34;&gt;list&lt;/span&gt;()

&lt;span class=&#34;pl-k&#34;&gt;for&lt;/span&gt; p &lt;span class=&#34;pl-k&#34;&gt;in&lt;/span&gt; prior:
    sim_data.append(generative_model(p))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we are going to arrive at our posterior distribution by selecting only the values from our prior distribution that generated instances where the simulated result matched our observed result of 10 purchases.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;posterior &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; prior[&lt;span class=&#34;pl-c1&#34;&gt;list&lt;/span&gt;(&lt;span class=&#34;pl-c1&#34;&gt;map&lt;/span&gt;(&lt;span class=&#34;pl-k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;pl-smi&#34;&gt;x&lt;/span&gt;: x &lt;span class=&#34;pl-k&#34;&gt;==&lt;/span&gt; observed, sim_data))]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Below is what our posterior distribution looks like.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;posterior.hist()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/467a89798b1010d322b31f5f0e601ef3117e3135/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f706f73746572696f722d646973747269627574696f6e2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/467a89798b1010d322b31f5f0e601ef3117e3135/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f706f73746572696f722d646973747269627574696f6e2e706e67&#34; alt=&#34;Posterior Distribution&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/posterior-distribution.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see just by looking at this that the updated probability of someone making a purchase from our online store is likely between 10% and 30% and most likely somewhere between 17% and 20%.&lt;/p&gt;
&lt;p&gt;We can also calculate some statistics for our posterior distribution in addition to visualizing it. This will show us the mean, range, and standard deviation of our posterior distribution in addition to the quartiles.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;posterior.describe()

count    &lt;span class=&#34;pl-c1&#34;&gt;2040.000000&lt;/span&gt;
mean        &lt;span class=&#34;pl-c1&#34;&gt;0.211445&lt;/span&gt;
std         &lt;span class=&#34;pl-c1&#34;&gt;0.055113&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;min&lt;/span&gt;         &lt;span class=&#34;pl-c1&#34;&gt;0.067276&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;%&lt;/span&gt;         &lt;span class=&#34;pl-c1&#34;&gt;0.172084&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;%&lt;/span&gt;         &lt;span class=&#34;pl-c1&#34;&gt;0.207662&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;75&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;%&lt;/span&gt;         &lt;span class=&#34;pl-c1&#34;&gt;0.247589&lt;/span&gt;
&lt;span class=&#34;pl-c1&#34;&gt;max&lt;/span&gt;         &lt;span class=&#34;pl-c1&#34;&gt;0.405344&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can see that both the mean and the median are approximately 21%. If we wanted to, we could also calculate a 90% credible interval for this distribution as follows.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;&lt;span class=&#34;pl-c1&#34;&gt;print&lt;/span&gt;(posterior.quantile(&lt;span class=&#34;pl-c1&#34;&gt;.025&lt;/span&gt;), &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;|&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, posterior.quantile(&lt;span class=&#34;pl-c1&#34;&gt;.975&lt;/span&gt;))

&lt;span class=&#34;pl-c1&#34;&gt;0.11580212180759529&lt;/span&gt; &lt;span class=&#34;pl-k&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;pl-c1&#34;&gt;0.3279022489553774&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we could also round the values in the posterior distribution to whole percents and calculate the most likely one along with its probability. This is known as the &lt;em&gt;maximum likelihood estimate&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;rounded &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; posterior.round(&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;)
mode &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; rounded.mode()[&lt;span class=&#34;pl-c1&#34;&gt;0&lt;/span&gt;]
probability &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;pl-c1&#34;&gt;list&lt;/span&gt;(rounded).count(mode)&lt;span class=&#34;pl-k&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;len&lt;/span&gt;(rounded)
&lt;span class=&#34;pl-c1&#34;&gt;print&lt;/span&gt;(&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;Maximum Likelihood Estimate: &lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, mode, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;|&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;,probability)

Maximum Likelihood Estimate:  &lt;span class=&#34;pl-c1&#34;&gt;0.19&lt;/span&gt; &lt;span class=&#34;pl-k&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;pl-c1&#34;&gt;0.07549019607843137&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This tells us that given the data we have thus far, it is most likely that 19% of the visitors to our online store will make a purchase.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-summary&#34; class=&#34;anchor&#34; href=&#34;#summary&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this lesson, we covered the basics of Bayesian statistics and Bayesian data analysis. We began the lesson with showing how conditional probabilities form the basis for Bayesian statistics. We then introduced Bayes&#39; Theorem and looked at a simple example of how to use the formula to arrive at an estimate. From there, we demonstrated how to perform a Bayesian analysis using a real-world scenario and how to calculate statistics for the resulting posterior distribution. One important thing to note is that the approach we used for this analysis is called &lt;em&gt;Approximate Bayesian Computation (ABC)&lt;/em&gt;. It is conceptually easy to understand (which is why we chose it) but computationally intensive since we generated a large number of simulated results and then filtered them. There are other ways to do this that are faster but are much less intuitive, so we decided to stick with ABC.&lt;/p&gt;

&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_contents_9"
    aria-labelledby="tab_9"
    aria-hidden="true"
    class="seq_contents tex2jax_ignore asciimath2jax_ignore">
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-vertical&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;VerticalStudentView&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@d0efa5d38bbf43db927f3753ce86aa8c&#34; data-block-type=&#34;vertical&#34;&gt;
  

  &lt;h2 class=&#34;hd hd-2 unit-title&#34;&gt;Principal Component Analysis&lt;/h2&gt;

    &lt;div class=&#34;ih-bookmark-button-wrapper&#34;&gt;
    	



&lt;div class=&#34;bookmark-button-wrapper&#34;&gt;
  &lt;button class=&#34;btn btn-link bookmark-button &#34;
    aria-pressed=&#34;false&#34;
    data-bookmark-id=&#34;NoahB,block-v1:IRONHACK+DAFT+201906_MIA+type@vertical+block@d0efa5d38bbf43db927f3753ce86aa8c&#34;
    data-bookmarks-api-url=&#34;/api/bookmarks/v1/bookmarks/&#34;&gt;
     &lt;span class=&#34;bookmark-text&#34;&gt;Bookmark this page&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

    &lt;/div&gt;	

&lt;div class=&#34;vert-mod&#34;&gt;
  &lt;div class=&#34;vert vert-0&#34; data-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@04a4b9a2bb604490a0d497ca4f465c43&#34;&gt;
    &lt;div class=&#34;xblock xblock-student_view xblock-student_view-html xmodule_display xmodule_HtmlModule&#34; data-runtime-class=&#34;LmsRuntime&#34; data-init=&#34;XBlockToXModuleShim&#34; data-block-type=&#34;html&#34; data-request-token=&#34;655e2bf8939911e99a866e487447f8c8&#34; data-runtime-version=&#34;1&#34; data-usage-id=&#34;block-v1:IRONHACK+DAFT+201906_MIA+type@html+block@04a4b9a2bb604490a0d497ca4f465c43&#34; data-type=&#34;HTMLModule&#34; data-course-id=&#34;course-v1:IRONHACK+DAFT+201906_MIA&#34;&gt;
  &lt;script type=&#34;json/xblock-args&#34; class=&#34;xblock-json-init-args&#34;&gt;
    {&#34;xmodule-type&#34;: &#34;HTMLModule&#34;}
  &lt;/script&gt;
  &lt;h2&gt;
&lt;a id=&#34;user-content-lesson-goals&#34; class=&#34;anchor&#34; href=&#34;#lesson-goals&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Lesson Goals&lt;/h2&gt;
&lt;p&gt;In this lesson we will learn how to reduce the dimensions of our data using a technique called PCA. We will learn the theory behind this technique as well as how to implement it in Python.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-introduction&#34; class=&#34;anchor&#34; href=&#34;#introduction&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;PCA is a dimensionality reduction technique that is used to transform our data. What motivates us to reduce the dimensions of our data?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We may want to speed up the performance of our model by providing it with less variables&lt;/li&gt;
&lt;li&gt;We may have many columns of very sparse data. By using PCA, we have fewer columns with less sparsity. This may improve the performance of our model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Creating less variables that are a function of our original data enables us to accomplish these goals.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-an-introduction-to-dimensions&#34; class=&#34;anchor&#34; href=&#34;#an-introduction-to-dimensions&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;An Introduction to Dimensions&lt;/h2&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-one-dimensional-charts&#34; class=&#34;anchor&#34; href=&#34;#one-dimensional-charts&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;One Dimensional Charts&lt;/h3&gt;
&lt;p&gt;Let&#39;s say we have some one dimensional data. Plotting it could still give us useful information. For example, the chart below shows us uniformly distributed data&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/ba77b6bc1322d5f45dc2cce390a21bf4080917bd/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6f6e65642d756e69666f726d2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/ba77b6bc1322d5f45dc2cce390a21bf4080917bd/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6f6e65642d756e69666f726d2e706e67&#34; alt=&#34;one dimensional uniform&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/oned-uniform.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, we can also use one dimensional charts to show data that is non uniform.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/3817ed238a5d43e55f23fc2bbedf0278aafab3eb/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6f6e65642d6d756c74692d6d6f64652e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/3817ed238a5d43e55f23fc2bbedf0278aafab3eb/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6f6e65642d6d756c74692d6d6f64652e706e67&#34; alt=&#34;one dimensional multi modal&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/oned-multi-mode.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-two-dimensional-charts&#34; class=&#34;anchor&#34; href=&#34;#two-dimensional-charts&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Two Dimensional Charts&lt;/h3&gt;
&lt;p&gt;By adding a second feature to our data, we are able to plot it in 2 dimensional space.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/8e4891d9a0e9dfc6832482485df8dab35ea65357/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f74776f642d706c6f742e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/8e4891d9a0e9dfc6832482485df8dab35ea65357/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f74776f642d706c6f742e706e67&#34; alt=&#34;two d scatter chart&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/twod-plot.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-three-dimensional-charts&#34; class=&#34;anchor&#34; href=&#34;#three-dimensional-charts&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Three Dimensional Charts&lt;/h3&gt;
&lt;p&gt;We could also go further and plot our data in three dimensions. However, this is where our visualizations stop.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/6c82a2f6d44f8d2ecdc919982dbb0690a019483d/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7468726565642d736361747465722e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/6c82a2f6d44f8d2ecdc919982dbb0690a019483d/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7468726565642d736361747465722e706e67&#34; alt=&#34;three d scatter chart&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/threed-scatter.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;However, in reality, our data has more than 3 dimensions.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-flattening-the-data&#34; class=&#34;anchor&#34; href=&#34;#flattening-the-data&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Flattening the Data&lt;/h2&gt;
&lt;h3&gt;
&lt;a id=&#34;user-content-back-to-2-d&#34; class=&#34;anchor&#34; href=&#34;#back-to-2-d&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Back to 2-D&lt;/h3&gt;
&lt;p&gt;Suppose we have a two dimensional dataset plotted below.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/336cf71a707ed0df81fca4e35be26ff16701b51e/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6d616a6f726974792d6f662d766172696174696f6e2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/336cf71a707ed0df81fca4e35be26ff16701b51e/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6d616a6f726974792d6f662d766172696174696f6e2e706e67&#34; alt=&#34;majority of variation&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/majority-of-variation.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If we flattened the data, it wouldn&#39;t look much different&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/b7d387d6261a7f2ac969c5b643c15206f69923fc/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f666c617474656e65642d32642e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/b7d387d6261a7f2ac969c5b643c15206f69923fc/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f666c617474656e65642d32642e706e67&#34; alt=&#34;flattened 2d chart&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/flattened-2d.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this case, we can move the data from two to one dimensions without much information loss. This new visualization still conveys the information that the majority of the variation in the data is on the x axis.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/f8975634ba2e02f53b23399af420c03849bafa35/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7265647563652d746f2d31642e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/f8975634ba2e02f53b23399af420c03849bafa35/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7265647563652d746f2d31642e706e67&#34; alt=&#34;reduce to 1-D&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/reduce-to-1d.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-dimension-reduction&#34; class=&#34;anchor&#34; href=&#34;#dimension-reduction&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Dimension Reduction&lt;/h2&gt;
&lt;p&gt;In real life, we reduce the dimensions of things quite often while still maintaining most of the important information. For example, we are able to watch television and still understand the images even though they have been reduced from 3 dimensions to 2 dimensions. PCA is meant to reduce data such that we still retain a large amount of information about the data. Below is an example of how variation looks in 2 dimensional data.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/dabd1465dc96a4ac32b3fc71b7c3575aec7de91d/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f32642d766172696174696f6e2e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/dabd1465dc96a4ac32b3fc71b7c3575aec7de91d/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f32642d766172696174696f6e2e706e67&#34; alt=&#34;2d variation&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/2d-variation.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-changing-the-axes&#34; class=&#34;anchor&#34; href=&#34;#changing-the-axes&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Changing the Axes&lt;/h2&gt;
&lt;p&gt;Imagine if we rotated the axes. Now the variation acts as our new x and y axes. After rotating, it&#39;s easy to see the directions of variation in the data.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/e9178335e8ddf170521821cd2b3493c573fd8b8c/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f726f746174652d617865732e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/e9178335e8ddf170521821cd2b3493c573fd8b8c/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f726f746174652d617865732e706e67&#34; alt=&#34;axes rotation&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/rotate-axes.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;These two new axes are called the principal components of the data. The first component (in blue) spans the most variation, while the second component (in green) spans the second most variation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/9d4eccc2021ef60f36b1657daf2b268b2e827d63/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7072696e636970616c2d617865732e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/9d4eccc2021ef60f36b1657daf2b268b2e827d63/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f7072696e636970616c2d617865732e706e67&#34; alt=&#34;principal axes&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/principal-axes.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can perform this methodology with more dimensions and rank more than two components.&lt;/p&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-pca-in-python&#34; class=&#34;anchor&#34; href=&#34;#pca-in-python&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;PCA in Python&lt;/h2&gt;
&lt;p&gt;We will find the principal components in our data using the &lt;code&gt;PCA&lt;/code&gt; function in &lt;code&gt;statsmodels&lt;/code&gt;. In this example, we will be using &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/data/module-2/breast-cancer.csv&#34; rel=&#34;nofollow&#34;&gt;a breast cancer dataset&lt;/a&gt; from the UCI data repository. This data contains 35 columns, we will retain 31 numeric columns and find the top 2 principal components of our data.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;total_cols &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; [&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;id&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;outcome&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;time&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_radius&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_texture&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_perimiter&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;,
              &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_area&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_smoothness&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_compactness&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_concavity&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;,
              &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_concave_points&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_symmetry&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_1_fractal_dimension&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;,
              &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_radius&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_texture&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_perimiter&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_area&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;,
              &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_smoothness&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_compactness&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_concavity&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;,
              &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_concave_points&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_symmetry&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_2_fractal_dimension&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_radius&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, 
              &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_texture&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_perimiter&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_area&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_smoothness&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;,
              &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_compactness&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_concavity&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_concave_points&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_symmetry&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;,
              &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;cell_3_fractal_dimension&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;tumor_size&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;lymph_status&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;]
breast_cancer &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; pd.read_csv(&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;breast-cancer.csv&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-v&#34;&gt;names&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;total_cols)
breast_cancer.head()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://camo.githubusercontent.com/e23ebf2820b8f4ae01f17adfd641410803fda75b/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6272656173742d63616e6365722d686561642e706e67&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/e23ebf2820b8f4ae01f17adfd641410803fda75b/68747470733a2f2f73332d65752d776573742d312e616d617a6f6e6177732e636f6d2f69682d6d6174657269616c732f75706c6f6164732f646174612d7374617469632f696d616765732f6272656173742d63616e6365722d686561642e706e67&#34; alt=&#34;breast cancer head&#34; data-canonical-src=&#34;https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/images/breast-cancer-head.png&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now we will extract only the numeric columns and find the principal components using the &lt;code&gt;PCA&lt;/code&gt; function. For this example, we will limit ourselves to only 2 components.&lt;/p&gt;
&lt;div class=&#34;highlight highlight-source-python&#34;&gt;&lt;pre&gt;numeric_columns &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; [x &lt;span class=&#34;pl-k&#34;&gt;for&lt;/span&gt; x &lt;span class=&#34;pl-k&#34;&gt;in&lt;/span&gt; total_cols &lt;span class=&#34;pl-k&#34;&gt;if&lt;/span&gt; x &lt;span class=&#34;pl-k&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;pl-k&#34;&gt;in&lt;/span&gt; [&lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;id&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;outcome&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;time&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&#34;pl-s&#34;&gt;&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;lymph_status&lt;span class=&#34;pl-pds&#34;&gt;&#39;&lt;/span&gt;&lt;/span&gt;]]
breast_cancer_numeric &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; breast_cancer[numeric_columns]
pc &lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt; PCA(np.array(breast_cancer_numeric), &lt;span class=&#34;pl-v&#34;&gt;ncomp&lt;/span&gt;&lt;span class=&#34;pl-k&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;)
pc.factors.shape
(&lt;span class=&#34;pl-c1&#34;&gt;198&lt;/span&gt;, &lt;span class=&#34;pl-c1&#34;&gt;2&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;
&lt;a id=&#34;user-content-conclusion&#34; class=&#34;anchor&#34; href=&#34;#conclusion&#34; aria-hidden=&#34;true&#34;&gt;&lt;span aria-hidden=&#34;true&#34; class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this lesson we learned mainly about the intuition behind PCA. We learned that PCA is used mainly to deal with data with a large number of features. PCA helps us improve the speed and sometimes the accuracy of our ML algorithms. The main goal in PCA is to reduce the dimensions of the data while primarily preserving the variation in the data.&lt;/p&gt;

&lt;/div&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

  </div>
  <div id="seq_content" class="ih-learnig-unit" role="tabpanel"></div>

  <nav class="sequence-bottom ih-js-fire-highlighting-controls" aria-label="Section">
    <button class="sequence-nav-button button-previous">
      <span class="icon fa fa-chevron-prev" aria-hidden="true"></span>
      <span>Previous</span>
    </button>
    <button class="sequence-nav-button button-next">
      <span>Next</span>
      <span class="icon fa fa-chevron-next" aria-hidden="true"></span>
    </button>
  </nav>
</div>

</div>

        </main>
    </section>

    <section class="courseware-results-wrapper">
      <div id="loading-message" aria-live="polite" aria-relevant="all"></div>
      <div id="error-message" aria-live="polite"></div>
      <div class="courseware-results search-results" data-course-id="course-v1:IRONHACK+DAFT+201906_MIA" data-lang-code="en"></div>
    </section>

  </div>
  









    
    
    <script type="text/javascript" src="../../../../../static/bundles/CourseSock.43ffb68a30fa51e4e467.1f398818dd61.js" ></script>
      <script type="text/javascript">
        
    new CourseSock({
        el:'.verification-sock'
    });

      </script>


</div>
<div class="container-footer">
</div>

          
        </div>
      </div>

    
    

      <div class="ih-beta-banner">
        <span class="icon fa fa-flask"></span>
        <span class="ih-beta-banner-text">We just launched a new version of the student platform. Everything should be fine, but if you encounter a bug, don't hesitate and send it our way.</span>
      </div>

  </div>

  
  
  <script type="text/javascript" src="../../../../../static/common/js/vendor/jquery.scrollTo.525edcc52fe8.js"></script>
  <script type="text/javascript" src="../../../../../static/js/vendor/flot/jquery.flot.d3d45ff0c6a8.js"></script>

  
    <script type="text/javascript" src="../../../../../static/js/lms-courseware.43ed60564822.js" charset="utf-8"></script>


  



<script type="text/javascript">
    // Fast Preview was introduced in 2.5. However, it
    // causes undesirable flashing/font size changes when
    // MathJax is used for interactive preview (equation editor).
    // Setting processSectionDelay to 0 (see below) fully eliminates
    // fast preview, but to reduce confusion, we are also setting
    // the option as displayed in the context menu to false.
    // When upgrading to 2.6, check if this variable name changed.
    window.MathJax = {
      menuSettings: {CHTMLpreview: false}
    };
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [
        ["\\(","\\)"],
        ['[mathjaxinline]','[/mathjaxinline]']
      ],
      displayMath: [
        ["\\[","\\]"],
        ['[mathjax]','[/mathjax]']
      ]
    }
  });
</script>
<script type="text/x-mathjax-config">

  // In order to eliminate all flashing during interactive
  // preview, it is necessary to set processSectionDelay to 0
  // (remove delay between input and output phases). This
  // effectively disables fast preview, regardless of
  // the fast preview setting as shown in the context menu.
  MathJax.Hub.processSectionDelay = 0;

  MathJax.Hub.signal.Interest(function(message) {
    if(message[0] === "End Math") {
        set_mathjax_display_div_settings();
    }
  });
  function set_mathjax_display_div_settings() {
    $('.MathJax_Display').each(function( index ) {
      this.setAttribute('tabindex', '0');
      this.setAttribute('aria-live', 'off');
      this.removeAttribute('role');
      this.removeAttribute('aria-readonly');
    });
  }
</script>


<!-- This must appear after all mathjax-config blocks, so it is after the imports from the other templates.
     It can't be run through static.url because MathJax uses crazy url introspection to do lazy loading of
     MathJax extension libraries -->
<script type="text/javascript" src="../../../../../../cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJaxf4fa.js?config=TeX-MML-AM_SVG"></script>


    
    
      <script type="text/javascript" src="../../../../../static/course_search/js/course_search_factory.26a9d7197c46bdd1.js?raw"></script>
    <script type="text/javascript">
        (function (require) {
            require(['course_search/js/course_search_factory'], function (CourseSearchFactory) {
                
        var courseId = $('.courseware-results').data('courseId');
        CourseSearchFactory({
            courseId: courseId,
            searchHeader: $('.search-bar')
        });
    
            });
        }).call(this, require || RequireJS.require);
    </script>


  
    
      <script type="text/javascript" src="../../../../../static/js/courseware/courseware_factory.1504fc10caefbdd1.js?raw"></script>
    <script type="text/javascript">
        (function (require) {
            require(['js/courseware/courseware_factory'], function (CoursewareFactory) {
                
    CoursewareFactory();
  
            });
        }).call(this, require || RequireJS.require);
    </script>



  <script type="text/javascript">
    var $$course_id = "course\u002Dv1:IRONHACK+DAFT+201906_MIA";
  </script>






  



  <script type="text/javascript" src="../../../../../static/js/vendor/noreferrer.aa62a3e70ffa.js" charset="utf-8"></script>
  <script type="text/javascript" src="../../../../../static/js/utils/navigation.08930e16ab3d.js" charset="utf-8"></script>
  <script type="text/javascript" src="../../../../../static/js/header/header.309a1243e175.js"></script>
  

  

    <script type="text/javascript" src="../../../../../static/ih-lms-theme/js/vendor/highlight.min.496e109b43ca.js" ></script>
    <script type="text/javascript" src="../../../../../static/ih-lms-theme/js/vendor/ih-highlight.0f32f131ba83.js" ></script>
  
</body>
</html>


